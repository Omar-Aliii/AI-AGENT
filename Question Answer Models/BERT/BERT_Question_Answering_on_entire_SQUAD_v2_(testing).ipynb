{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omar-Aliii/AI-AGENT/blob/main/Question%20Answer%20Models/BERT/BERT_Question_Answering_on_entire_SQUAD_v2_(testing).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers: This library by Hugging Face provides a collection of pre-trained models for Natural Language Processing (NLP) tasks, including BERT, GPT, RoBERTa, and more. It also includes tools for training your own models and fine-tuning existing ones."
      ],
      "metadata": {
        "id": "FCQbcIIEkZ07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "321212da-c185-4df6-d0bc-e77337412cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET1wfaBF9Li2"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU1cyFC69Li2",
        "outputId": "54dd2d65-c31d-4175-b028-63101c0e82b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "266152b720ce4abaa7178cdb553f379e",
            "9baea6eae06c4eebb17f5c03545d125c",
            "0c5c053104b748199540d81c55a04bda",
            "17238d55309a404b852cc758f663cd9d",
            "e1b7e337da704b8497aa71032c303f6a",
            "98cf8d173a8f4239abd319b5dfbb3b27",
            "cd5a628644bd485a8aea850ea07ddf82",
            "bc84ec8b69114b918fd4acb5b6e2d99d",
            "1ce01442ccb2498da4cf2f0c2cdc04cb",
            "21cefd284b494db894cdfe3278c0d0d4",
            "af424ce0e72c497aa5ee6140e86cba89",
            "2fb70996ce82416193a2ec3c97cf8b21",
            "338027b315e149a5bca2817ed0d2ab13",
            "a56d55e380bf4a4987f987028bb8ccdd",
            "4061a372dbb8436581891712a930f969",
            "221bc73560e64439a77864c3aaeaea52",
            "7527aba3fe94471e8eaa313ff2ab9025"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "266152b720ce4abaa7178cdb553f379e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbaL0CNX9Li3"
      },
      "source": [
        "Then you need to install Git-LFS. Uncomment the following instructions:\n",
        "\n",
        "Git LFS is an extension to Git that allows managing large files efficiently. It replaces large files in your repository with text pointers inside Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh-zKawM9Li3",
        "outputId": "bd0175b1-043a-4cf0-ecac-cc7cdb6b3e85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBUkOTMt9Li4"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-QMnsSV9Li4",
        "outputId": "d244b42c-b662-4753-9cf9-7db070969a00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/question-answering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh8hYpXc9Li5"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import send_example_telemetry\n",
        "#send information to hugging face\n",
        "send_example_telemetry(\"question_answering_notebook\", framework=\"pytorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "send_example_telemetry function you're using is part of the Hugging Face Transformers library, and **it is used to send telemetry data about model usage to Hugging Face**. This helps them gather information about how their models are being used in the community."
      ],
      "metadata": {
        "id": "Q97C5zt-lrgX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a question-answering task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
        "# answers are allowed or not).\n",
        "\n",
        "squad_v2 = False\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "squad_v2: This flag indicates whether you're working with SQuAD version 2 (True) or version 1 (False). SQuAD v2 introduced questions where the answer is not present in the provided passage, making it a more challenging dataset. If squad_v2 is False, it suggests that you're working with SQuAD v1, where every question has a corresponding answer in the passage.\n",
        "\n",
        "**model_checkpoint:** This variable specifies the model checkpoint or pre-trained model you want to use for fine-tuning. In this case, it's set to \"distilbert-base-uncased,\" which is a smaller and faster version of BERT (Bidirectional Encoder Representations from Transformers) pre-trained on uncased text. You can replace this with other model checkpoints available in the Hugging Face model hub\n",
        "\n",
        "**batch_size**:  It determines how many examples are processed in each iteration during training.\n",
        " model will compute gradients and update its parameters based on the average loss calculated over these 16 examples\n",
        "\n",
        "  batch size often depends on various factors, including the **available memory on the GPU or CPU, the size of the dataset**"
      ],
      "metadata": {
        "id": "fkSEcR9gm5zt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`load_dataset function`**:\n",
        "\n",
        "This function is used to load datasets from the Hugging Face datasets library.\n",
        "\n",
        "**`load_metric function:`**\n",
        "\n",
        "This function is used to load evaluation metrics from the datasets library.\n",
        "\n",
        "\n",
        "these metrics provide a way to quantify how well a model is performing in terms of its predictions compared to the ground truth or correct answers."
      ],
      "metadata": {
        "id": "iN9s2bSVng8m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "For our example here, we'll use the [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/). The notebook should work with any question answering dataset provided by the ðŸ¤— Datasets library. If you're using your own dataset defined from a JSON or csv file (see the [Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) on how to load them), it might need some adjustments in the names of the columns used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_AY1ATSIrIq"
      },
      "outputs": [],
      "source": [
        "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWiVUF0jIrIv",
        "outputId": "2c9352c6-022a-4766-922f-c0ac324187dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFgwHAZe9Li9"
      },
      "source": [
        "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "datasets[\"train\"][0] is used to access the first example in the training split of your SQuAD dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6HrpprwIrIz",
        "outputId": "89890217-51cf-4e70-ba2a-66c725f08a43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer_start': [515]:** the answer of the question start with character at position 515"
      ],
      "metadata": {
        "id": "cO-8CjbhrmM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "jBdFomQ-r76z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "show_random_elements that takes a dataset and displays a specified number of randomly picked examples. The function decodes labels if they are encoded (e.g., if they are represented as integer indices), making it easier to understand the content of the dataset\n",
        "\n",
        "\n",
        "**how the function works:**\n",
        "\n",
        "It randomly selects indices to pick examples from the dataset, ensuring that\n",
        "\n",
        "the same example is not picked more than once.\n",
        "\n",
        "It creates a Pandas DataFrame from the selected examples.\n",
        "\n",
        "It decodes class labels if they are represented as integer indices.\n",
        "\n",
        "It displays the DataFrame for better tabular representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "#The dataset from which random elements will be displayed.\n",
        "# The number of random examples to display (default is 10).\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZy5tRB_IrI7",
        "outputId": "19964f78-31ce-408c-868e-a9c6533bcccb",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5732ac1fcc179a14009dabe7</td>\n",
              "      <td>Geological_history_of_Earth</td>\n",
              "      <td>During the Miocene continents continued to drift toward their present positions. Of the modern geologic features, only the land bridge between South America and North America was absent, the subduction zone along the Pacific Ocean margin of South America caused the rise of the Andes and the southward extension of the Meso-American peninsula. India continued to collide with Asia. The Tethys Seaway continued to shrink and then disappeared as Africa collided with Eurasia in the Turkish-Arabian region between 19 and 12 Ma (ICS 2004). Subsequent uplift of mountains in the western Mediterranean region and a global fall in sea levels combined to cause a temporary drying up of the Mediterranean Sea resulting in the Messinian salinity crisis near the end of the Miocene.</td>\n",
              "      <td>Which continent was India colliding with in the Miocene?</td>\n",
              "      <td>{'text': ['Asia'], 'answer_start': [376]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56e19df2e3433e1400423035</td>\n",
              "      <td>Catalan_language</td>\n",
              "      <td>Standard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of Valencia and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia.</td>\n",
              "      <td>What form is excepted by most speakers?</td>\n",
              "      <td>{'text': ['Standard Catalan'], 'answer_start': [0]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56ddf1d166d3e219004dae41</td>\n",
              "      <td>Institute_of_technology</td>\n",
              "      <td>In the French-speaking part of Switzerland exists also the term haute Ã©cole specialisÃ©e for a type of institution called Fachhochschule in the German-speaking part of the country. (see below).</td>\n",
              "      <td>What's the term in German for what those in French-speaking Switzerland call haute Ã©cole specialisÃ©e?</td>\n",
              "      <td>{'text': ['Fachhochschule'], 'answer_start': [121]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56de91054396321400ee2a1f</td>\n",
              "      <td>Arnold_Schwarzenegger</td>\n",
              "      <td>In January 2011, just weeks after leaving office in California, Schwarzenegger announced that he was reading several new scripts for future films, one of them being the World War II action drama With Wings as Eagles, written by Randall Wallace, based on a true story. On March 6, 2011, at the Arnold Seminar of the Arnold Classic, Schwarzenegger revealed that he was being considered for several films, including sequels to The Terminator and remakes of Predator and The Running Man, and that he was \"packaging\" a comic book character. The character was later revealed to be the Governator, star of the comic book and animated series of the same name. Schwarzenegger inspired the character and co-developed it with Stan Lee, who would have produced the series. Schwarzenegger would have voiced the Governator.</td>\n",
              "      <td>Schwarzenegger said he was reading scripts that included the one for what WWII film based on a true story?</td>\n",
              "      <td>{'text': ['With Wings as Eagles'], 'answer_start': [195]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5735bb89e853931400426af8</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The Pashupatinath Temple is a famous 5th century Hindu temple dedicated to Lord Shiva (Pashupati). Located on the banks of the Bagmati River in the eastern part of Kathmandu, Pashupatinath Temple is the oldest Hindu temple in Kathmandu. It served as the seat of national deity, Lord Pashupatinath, until Nepal was secularized. However, a significant part of the temple was destroyed by Mughal invaders in the 14th century and little or nothing remains of the original 5th-century temple exterior. The temple as it stands today was built in the 19th century, although the image of the bull and the black four-headed image of Pashupati are at least 300 years old. The temple is a UNESCO World Heritage Site. Shivaratri, or the night of Lord Shiva, is the most important festival that takes place here, attracting thousands of devotees and sadhus.[citation needed]</td>\n",
              "      <td>What faith does the Pashupatinath Temple serve?</td>\n",
              "      <td>{'text': ['Hindu'], 'answer_start': [49]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>56fb2e3bf34c681400b0c1f9</td>\n",
              "      <td>Middle_Ages</td>\n",
              "      <td>The Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350, the Black Death killed about a third of Europeans. Controversy, heresy, and schism within the Church paralleled the interstate conflict, civil strife, and peasant revolts that occurred in the kingdoms. Cultural and technological developments transformed European society, concluding the Late Middle Ages and beginning the early modern period.</td>\n",
              "      <td>Along with controversy and schism, what upset the peace of the Church during the Late Middle Ages?</td>\n",
              "      <td>{'text': ['heresy'], 'answer_start': [242]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>57290ceeaf94a219006a9fd8</td>\n",
              "      <td>Race_(human_categorization)</td>\n",
              "      <td>One crucial innovation in reconceptualizing genotypic and phenotypic variation was the anthropologist C. Loring Brace's observation that such variations, insofar as it is affected by natural selection, slow migration, or genetic drift, are distributed along geographic gradations or clines. In part this is due to isolation by distance. This point called attention to a problem common to phenotype-based descriptions of races (for example, those based on hair texture and skin color): they ignore a host of other similarities and differences (for example, blood type) that do not correlate highly with the markers for race. Thus, anthropologist Frank Livingstone's conclusion, that since clines cross racial boundaries, \"there are no races, only clines\".</td>\n",
              "      <td>Why are variations distributed along clines?</td>\n",
              "      <td>{'text': ['isolation by distance'], 'answer_start': [314]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>56df680f8bc80c19004e4bdd</td>\n",
              "      <td>Hunter-gatherer</td>\n",
              "      <td>Many groups continued their hunter-gatherer ways of life, although their numbers have continually declined, partly as a result of pressure from growing agricultural and pastoral communities. Many of them reside in the developing world, either in arid regions or tropical forests. Areas that were formerly available to hunter-gatherers wereâ€”and continue to beâ€”encroached upon by the settlements of agriculturalists. In the resulting competition for land use, hunter-gatherer societies either adopted these practices or moved to other areas. In addition, Jared Diamond has blamed a decline in the availability of wild foods, particularly animal resources. In North and South America, for example, most large mammal species had gone extinct by the end of the Pleistoceneâ€”according to Diamond, because of overexploitation by humans, although the overkill hypothesis he advocates is strongly contested.[by whom?]</td>\n",
              "      <td>Where do many modern day hunter-gatherers live?</td>\n",
              "      <td>{'text': ['in the developing world'], 'answer_start': [211]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5726fe7ddd62a815002e9729</td>\n",
              "      <td>Crimean_War</td>\n",
              "      <td>The Allied army relocated without problems to the south and the heavy artillery was brought ashore with batteries and connecting trenches built so that by 10 October some batteries were ready and by 17 Octoberâ€”when the bombardment commencedâ€”126 guns were firing, 53 of them French.:430 The fleet at the same time engaged the shore batteries. The British bombardment worked better than the French, who had smaller caliber guns. The fleet suffered high casualties during the day. The British wanted to attack that afternoon, but the French wanted to defer the attack. A postponement was agreed, but on the next day the French were still not ready. By 19 October the Russians had transferred some heavy guns to the southern defenses and outgunned the allies.:431</td>\n",
              "      <td>When the continuous attack started, how many guns were firing?</td>\n",
              "      <td>{'text': ['126 guns'], 'answer_start': [241]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>57344da9acc1501500babde7</td>\n",
              "      <td>Hunting</td>\n",
              "      <td>Regional social norms are generally antagonistic to hunting, while a few sects, such as the Bishnoi, lay special emphasis on the conservation of particular species, such as the antelope. India's Wildlife Protection Act of 1972 bans the killing of all wild animals. However, the Chief Wildlife Warden may, if satisfied that any wild animal from a specified list has become dangerous to human life, or is so disabled or diseased as to be beyond recovery, permit any person to hunt such an animal. In this case, the body of any wild animal killed or wounded becomes government property.</td>\n",
              "      <td>What bans the killing of all wild animals in India?</td>\n",
              "      <td>{'text': ['Wildlife Protection Act of 1972'], 'answer_start': [195]}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Access the original dataset file\n",
        "\n",
        "train_dataset_path = squad_dataset['train'].cache_files[0] if 'train' in squad_dataset else None\n",
        "print(\"Path to the original SQuAD training dataset file:\", train_dataset_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "19mdxj3pI0O4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5424de5b-65d5-406c-8e41-d31faafce483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to the original SQuAD training dataset file: {'filename': '/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/squad-train.arrow'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`AutoTokenizer`**\n",
        "\n",
        " to load the appropriate tokenizer for a given pre-trained language model"
      ],
      "metadata": {
        "id": "g4Nj5ZVRiF3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2zMp5qC9Li-"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtdVPfbq9Li-"
      },
      "source": [
        "You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5hBlsrHIrJL",
        "outputId": "796e5bb6-b60e-4789-e3de-08dfb7bdb63e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oGAhymt9Li_"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the length is greater than a certain threshold (384 in this case), it may trigger actions like truncation or segmentation to fit the model's constraints."
      ],
      "metadata": {
        "id": "67pW4NLtl8-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gr1Mef29LjC"
      },
      "outputs": [],
      "source": [
        "for i, example in enumerate(datasets[\"train\"]):\n",
        "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
        "        break\n",
        "example = datasets[\"train\"][i]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code  is **iterating through the training examples in the dataset (datasets[\"train\"]) and finding the first example where the tokenized representation of the combined question and context exceeds a maximum length of 384 tokens**."
      ],
      "metadata": {
        "id": "f2TVqpE0kqlh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjblcIti9LjC"
      },
      "source": [
        "Without any truncation, we get the following length for the input IDs:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculates the length (number of tokens) of the tokenized input sequence for a specific example"
      ],
      "metadata": {
        "id": "Z0nCDeNWymTb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIlUgdtK9LjC",
        "outputId": "eecd56b6-892f-4080-d6d3-b2e5d1dff97c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "396"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculates the length of the tokenized representation of the question and context while taking into account the specified maximum length and truncation strategy.\n",
        "\n",
        "This is commonly used when you want to ensure that the combined length of the question and context does not exceed a certain limit (max_length)"
      ],
      "metadata": {
        "id": "iGXOimZd367Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WTez4hu9LjC",
        "outputId": "d18583ff-748b-4dce-a14a-65f6a37c920d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i1fGkb49LjD"
      },
      "source": [
        "Note that we never want to truncate the question, only the context, else the `only_second` truncation picked. Now, our tokenizer can automatically return us a list of features capped by a certain maximum length, with the overlap we talked above, we just have to tell it with `return_overflowing_tokens=True` and by passing the stride:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPkTOysQ9LjD"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`truncation`**\n",
        "\n",
        "refers to the process of shortening a sequence of text by removing a portion of it. This is often necessary when the original text is too long to fit within the maximum length constraints\n",
        "\n",
        "ex\n",
        "\n",
        "```\n",
        "# Question: \"What is the capital of France?\"\n",
        "\n",
        "Context: \"Paris, the beautiful capital of France, is known for its iconic landmarks...\"\n",
        "\n",
        "[CLS], \"What\", \"is\", \"the\", \"capital\", \"of\", \"France\", \"?\", [SEP], \"Paris\", \",\", \"the\", \"beautiful\", \"capital\", \"of\", \"France\", \",\", \"is\", \"known\", \"for\", \"its\", \"iconic\", \"landmarks\", \"...\", [SEP]\n",
        "\n",
        " truncation=\"only_second\"\n",
        "\n",
        "\n",
        "[CLS], \"What\", \"is\", \"the\", \"capital\", \"of\", \"France\", \"?\", [SEP], \"Paris\", \",\", \"the\", \"beautiful\", \"...\", [SEP]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I2ejYhIZnv3e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jc-Jy-N9LjD"
      },
      "source": [
        "Now we don't have one list of `input_ids`, but several:\n",
        "\n",
        "\n",
        "creates a list containing the length of each sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Uc9Ov59LjD",
        "outputId": "a86a7297-be4d-4419-9e11-c1b5b24dde4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[384, 157]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "[len(x) for x in tokenized_example[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KofuP_gV9LjD"
      },
      "source": [
        "And if we decode them, we can see the overlap:\n",
        "\n",
        "iterates over the first two tokenized input sequences in tokenized_example[\"input_ids\"] and prints the human-readable text by decoding each sequence using the tokenizer's decode method.\n",
        "\n",
        "Output:\n",
        "\n",
        "The human-readable text representation of each tokenized sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zesToo-P9LjD",
        "outputId": "e91fe5b2-2aa9-4048-90ef-f042888f0cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
            "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
          ]
        }
      ],
      "source": [
        "for x in tokenized_example[\"input_ids\"][:2]:\n",
        "    print(tokenizer.decode(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8boB4Ehz9LjD"
      },
      "source": [
        "Now this will give us some work to properly treat the answers: we need to find in which of those features the answer actually is, and where exactly in that feature. The models we will use require the start and end positions of these answers in the tokens, so we will also need to to map parts of the original context to some tokens. Thankfully, the tokenizer we're using can help us with that by returning an `offset_mapping`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmx93T2i9LjE",
        "outputId": "42ef769e-4a7c-4507-af2a-31e00c805ffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n"
          ]
        }
      ],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    stride=doc_stride\n",
        ")\n",
        "print(tokenized_example[\"offset_mapping\"][0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XBBChol9LjE"
      },
      "source": [
        "**`offset mapping`**\n",
        "\n",
        "**`provides the start and end character positions of the token in the original text.`**\n",
        "\n",
        " provides a mapping between the tokens in the tokenized sequence and their corresponding character positions in the original text.\n",
        "\n",
        "**stride=doc_stride**: This parameter determines the overlap between consecutive chunks when splitting a long document\n",
        "\n",
        "\n",
        "tokenizes the question and context of a specific example using the tokenizer with additional parameters for handling long documents or passages.\n",
        "\n",
        "**max_length=max_length**\n",
        "\n",
        "If the total number of tokens after tokenization exceeds this maximum length, the sequence is truncated or split accordingly\n",
        "\n",
        " \"**truncation**\" refers to the process of shortening a sequence of tokens to fit within a specified maximum length.\n",
        "\n",
        " \"**only_second**\" means that the truncation will be applied to the second input sequence (in this case, the \"context\").\n",
        "\n",
        "`**return_overflowing_tokens=True**`\n",
        "\n",
        "return any overflowing tokens that do not fit into the specified maximum length return_overflowing_tokens=True, the tokenizer will provide these additional parts, allowing you to handle cases where the input exceeds the model's maximum token limit.\n",
        "The additional parts are accessible in the overflowing_tokens field in the output.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okBL8lku9LjE",
        "outputId": "fe3cbf57-f6ee-4881-f519-fd607f275007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how How\n"
          ]
        }
      ],
      "source": [
        "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
        "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
        "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "first_token_id = tokenized_example[\"input_ids\"][0][1]:\n",
        "\n",
        "Retrieves the ID of the second token (index 1) in the first tokenized sequence. The index 0 often corresponds to the [CLS] token.\n",
        "offsets = tokenized_example\n",
        "\n",
        "[\"offset_mapping\"][0][1]:\n",
        "\n",
        "Retrieves the offset mapping for the second token in the first tokenized sequence. The offset mapping provides the start and end character positions of the token in the original text.\n",
        "\n",
        "tokenizer.convert_ids_to_tokens([first_token_id])[0] to convert the token ID back to its textual representation."
      ],
      "metadata": {
        "id": "iyL4EqTHtAqg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2WywB3X9LjE"
      },
      "source": [
        "So we can use this mapping to find the position of the start and end tokens of our answer in a given feature. We just have to distinguish which parts of the offsets correspond to the question and which part correspond to the context, this is where the `sequence_ids` method of our `tokenized_example` can be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy_zG21P9LjE",
        "outputId": "e31ce220-ebd2-43b9-9cff-53ac9cbc0a2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
          ]
        }
      ],
      "source": [
        "sequence_ids = tokenized_example.sequence_ids()\n",
        "print(sequence_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMgDuxp09LjE"
      },
      "source": [
        "**sequence_ids** method returns a list of sequence IDs for each token in the tokenized example\n",
        "\n",
        "It returns `None` for the special tokens,\n",
        "\n",
        "then **`0`** corresponding token comes from the first sentence past (the question)\n",
        "\n",
        " or **`1`** depending on whether the corresponding token comes from the first  the second (the context).\n",
        "\n",
        "  Now with all of this, we can find the first and last token of the answer in one of our input feature (or if the answer is not in this feature):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsMR7O689LjE",
        "outputId": "79026087-a953-44bf-f317-73d9e8182a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23 26\n"
          ]
        }
      ],
      "source": [
        "#Retrieve Answer Information:\n",
        "#nswers contains information about the answer, including the starting character position (start_char) and the ending character position (end_char).\n",
        "answers = example[\"answers\"]\n",
        "start_char = answers[\"answer_start\"][0]\n",
        "end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "# These while loops find the token indices corresponding to the start and end character positions of the answer in the tokenized sequence.\n",
        "token_start_index = 0\n",
        "while sequence_ids[token_start_index] != 1:\n",
        "    token_start_index += 1\n",
        "\n",
        "# End token index of the current span in the text.\n",
        "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
        "while sequence_ids[token_end_index] != 1:\n",
        "    token_end_index -= 1\n",
        "\n",
        "\n",
        "\n",
        "#send to omar\n",
        "# if the answer span is within the span of the tokenized sequence by comparing character positions with offset mappings\n",
        "offsets = tokenized_example[\"offset_mapping\"][0]\n",
        "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
        "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "        token_start_index += 1\n",
        "    start_position = token_start_index - 1\n",
        "    while offsets[token_end_index][1] >= end_char:\n",
        "        token_end_index -= 1\n",
        "    end_position = token_end_index + 1\n",
        "    print(start_position, end_position)\n",
        "else:\n",
        "    print(\"The answer is not in this feature.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuaowvsxYX88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "align the tokenized answer with the original text** by finding the corresponding token indices. It considers cases where the answer might extend beyond the boundaries of the feature"
      ],
      "metadata": {
        "id": "N6bb1CbSDFqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAzfTuDY9LjF"
      },
      "source": [
        "print the decoded representation of the answer span in the tokenized sequence and compare it with the original answer tex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xovCMGmg9LjF",
        "outputId": "68102d20-99ec-42fa-91cc-8c3ed307b5f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "over 1, 600\n",
            "over 1,600\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
        "print(answers[\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O529aVja9LjF"
      },
      "source": [
        "For this notebook to work with any kind of models, we need to account for the special case where the model expects padding on the left (in which case we switch the order of the question and the context):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In natural language processing tasks, when preparing input sequences for a model, padding is often applied to ensure that all sequences in a batch have the same length. The side on which the padding is applied can affect how the model processes the input. This variable, pad_on_right, can be used later in the code to handle cases where padding is applied on the right side of the input sequences."
      ],
      "metadata": {
        "id": "TZXsUQSzxM8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXKJb3k49LjF"
      },
      "outputs": [],
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0UQJHb39LjF"
      },
      "source": [
        "Now let's put everything together in one function we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the cls index for both the start and end position. We could also simply discard those examples from the training set if the flag `allow_impossible_answers` is `False`. Since the preprocessing is already complex enough as it is, we've kept is simple for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNJbMASp9LjF"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_subset = squad_subset.map(prepare_train_features, batched=True, remove_columns=squad_subset.column_names)\n"
      ],
      "metadata": {
        "id": "fZiHh16DdBr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " preparing data for training a question-answering model. It handles the tokenization, truncation, padding, and labeling of examples, taking into account the specifics of the tokenization process and the requirements of the downstream task"
      ],
      "metadata": {
        "id": "NIlpflf5LQcs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b70jh26IrJS"
      },
      "outputs": [],
      "source": [
        "features = prepare_train_features(datasets['train'][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare_train_features function is applied to the first 5 examples from the training dataset\n",
        "\n",
        "it processes and tokenizes a subset of training examples to prepare them for training a question-answering mode"
      ],
      "metadata": {
        "id": "yNthbeIQLmrT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command. Since our preprocessing changes the number of samples, we need to remove the old columns when applying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "# Assuming you want to transform the \"train\" subset\n",
        "# tokenized_datasets = datasets[\"train\"].map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "8ffdddf404e7471f90fd8a0d27f88047",
            "a673386a101f4f10bab319cd3e2d756e",
            "2a3c4cd174834cc2afaf4d01dc9f077c",
            "ac6ee5e07bca4c81bfe32c6515f823ce",
            "fb0ce3d2af154caf9127410fb343cd8b",
            "bb2c2688340642d9a9a8e3d26d3f4f76",
            "10edf8f29bdf4ab48737e1e8afed1d3a",
            "954c31c0daac44d3b60d2bbfaa69ea35",
            "842644a9ceb24d668fee97ae3fc5c6e2",
            "f4c65fdced464f619e737ab25091a944",
            "83128949f13648f58e8b0a58d822fdef"
          ]
        },
        "id": "TDTqP656nrja",
        "outputId": "1e9fdde1-be91-450b-c50d-1d9209254950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ffdddf404e7471f90fd8a0d27f88047"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Even better, the results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming datasets is already loaded\n",
        "print(len(datasets[\"train\"]), len(datasets[\"validation\"]))\n"
      ],
      "metadata": {
        "id": "6ILC51G4nELv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd78b890-25e9-48ff-8985-77068ecbe439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87599 10570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "percentage split is **`89.23% for training`** and **`10.77% for validation`** in the SQuAD dataset.\n",
        "\n",
        "\n",
        "https://www.tensorflow.org/datasets/catalog/squad#:~:text=Stanford%20Question%20Answering%20Dataset%20(SQuAD,the%20question%20might%20be%20unanswerable.&text=Versions%3A,3.0."
      ],
      "metadata": {
        "id": "eGKglkBDtT3P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`AutoModelForQuestionAnswering`**\n",
        "\n",
        "automatically load a pre-trained question-answering (QA) model based on a provided model identifier or path.\n",
        "\n",
        "**`from_pretrained`**\n",
        "\n",
        " method allows users to load models using either a model identifier or a local path. Model identifiers can be names like \"distilbert-base-uncased-distilled-squad\" or paths to locally stored models."
      ],
      "metadata": {
        "id": "yPT1dyMnGEVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW",
        "outputId": "a926b5bd-76fd-4563-d8f5-cd6097ecac76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "xEUcLM3cIAxm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`accelerate`** is a library developed by Hugging Face to simplify and optimize the training of deep learning models\n",
        "\n",
        "you can train your models on multiple GPUs or even multiple machines."
      ],
      "metadata": {
        "id": "b0AO_trAH8WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "7oXz2qMWBmro",
        "outputId": "bf9ab82d-6eaf-4f3a-9e16-71cbbdf783e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "To9g15q2Jd92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install transformers[torch] ensures that you have both the transformers library and the PyTorch dependencies installed\n",
        "\n",
        "[**`torch`**]: This is an extra specifier that indicates you want to install additional dependencies for PyTorch."
      ],
      "metadata": {
        "id": "uoQRMJHKJc2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "Wa2Xx2lKB6KF",
        "outputId": "dadd6f03-e2d8-49ce-95b4-9febe1bf4303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "VtJ89mAgC62L",
        "outputId": "737a14a6-a2f7-4bd1-90fd-344e27e172ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[pytorch]"
      ],
      "metadata": {
        "id": "YuOijaNxDpXZ",
        "outputId": "2682cb52-68da-4c36-c916-8e5374f3a288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[pytorch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "\u001b[33mWARNING: transformers 4.35.2 does not provide the extra 'pytorch'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[pytorch]) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[pytorch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[pytorch]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[pytorch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[pytorch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[pytorch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[pytorch]) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "\n",
        "accelerate.__version__"
      ],
      "metadata": {
        "id": "Lz3W1GibD-N3",
        "outputId": "6e0a2a40-88aa-4a94-d5c2-0a838567afd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.25.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "import transformers\n",
        "\n",
        "transformers.__version__, accelerate.__version__"
      ],
      "metadata": {
        "id": "wmvVcQ3jEG-f",
        "outputId": "40b50e63-e326-4ca2-a9f0-c520ad78ce38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('4.35.2', '0.25.0')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`TrainingArguments`** class. This class is part of the transformers library and is used to **store all the hyperparameters** and **settings for training a model.**\n",
        "\n",
        "**`evaluation_strategy=\"epoch\":`**  evaluation should be** performed at the end of each epoch** during training.\n",
        "\n",
        "**`learning rate`** is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
        "\n",
        "**`per_device_train_batch_size`** and **`per_device_eval_batch_size`** in the training configuration (TrainingArguments) are used to specify the batch size per device during training and evaluation, respectively"
      ],
      "metadata": {
        "id": "gd7b_PqPMM23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    #This is a formatted string that defines the directory where the fine-tuned model and related outputs will be saved.\n",
        "    f\"{model_name}-finetuned-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    #the speed at which a machine learning model \"learns\"\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    #a technique used to prevent overfitting,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay.\n",
        "\n",
        "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-squad\"` or `\"huggingface/bert-finetuned-squad\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jdPejSf9LjH"
      },
      "source": [
        "Then we will need a data collator that will batch our processed examples together, here the default one will work:\n",
        "\n",
        "**` data collator`** is a function or object responsible for combining individual training examples into batches. The purpose of a data collator is to take a list of samples and organize them into a batch that can be efficiently processed by the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKzcXkH79LjH"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "We will evaluate our model and compute metrics in the next section (this is a very long operation, so we will only compute the evaluation loss during training).\n",
        "\n",
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Trainer`** class in the transformers library provides a high-level interface for training and evaluating transformer models.\n",
        "\n",
        "Trainer simplifies the process of fine-tuning transformer models, making it easier for users to experiment with different architectures and hyperparameters\n",
        "\n",
        "**`Training Arguments (args`**): An object containing various hyperparameters and settings for training. This includes parameters such as learning rate, batch size, number of epochs, and more.\n",
        "\n",
        "**`Data Collator (data_collator)`**: A data collator is responsible for batching and organizing individual examples into input batches that can be processed by the model\n",
        "\n",
        "**`Tokenizer (tokenizer)`**: The tokenizer used to convert raw text into tokenized input suitable for the model. It is responsible for encoding text into input IDs and attention masks."
      ],
      "metadata": {
        "id": "ovc_GEE9TKEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets.split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrlL4oh_S5Vp",
        "outputId": "83acdcc6-3e5d-411b-b009-33cd724451d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19ku9H0VVMU-",
        "outputId": "3b9576d8-49a6-4b62-b787-82b7e2fe30dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
            "    num_rows: 88524\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets.split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNuEso0yViEe",
        "outputId": "d161d50e-5698-4aec-a50e-4301890d00d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "085cc75f-eb1e-4361-edd4-eca885235bf4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='33198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   18/33198 00:04 < 2:37:56, 3.50 it/s, Epoch 0.00/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 )\n\u001b[1;32m    368\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    370\u001b[0m                     \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mffn_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMbsX5ho9LjI"
      },
      "source": [
        "Since this training is particularly long, let's save the model just in case we need to restart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = [1, 2, 3]\n",
        "training_loss = [1.223600, 0.957300, 0.751500]\n",
        "validation_loss = [1.178746, 1.130409, 1.166829]\n",
        "\n",
        "# Plotting\n",
        "plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
        "plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEir4KfjXXT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6vADW1O9LjI"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QqGzIb89LjI"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9oUHM5q9LjI"
      },
      "source": [
        "Evaluating our model will require a bit more work, as we will need to map the predictions of our model back to parts of the context. The model itself predicts logits for the start and en position of our answers: if we take a batch from our validation datalaoder, here is the output our model gives us:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`perform evaluation on a batch of data.`**\n",
        "\n",
        "means using the trained model **to make predictions on a set of input examples** for the purpose of assessing the model's performance."
      ],
      "metadata": {
        "id": "x-J2OE_8CF0o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5nAQ10K9LjI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    output = trainer.model(**batch)\n",
        "output.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`logits`**\n",
        "\n",
        "They are the outputs of a neural network before applying an activation function, such as softmax. Logits can be seen as the scores assigned to each class in a classification problem or the unnormalized scores for different positions in a sequence."
      ],
      "metadata": {
        "id": "UsOxCIaXENb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking the shape of the logits produced by the model for start and end positions\n",
        " of answer in the context"
      ],
      "metadata": {
        "id": "8AaoqtfmDpBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Start Logits:`**\n",
        "\n",
        "indicate how likely it is to be the beginning of the answer.\n",
        "\n",
        "**`End Logits:`**\n",
        "\n",
        " indicate how likely it is to be the end of the answer.\n",
        "\n"
      ],
      "metadata": {
        "id": "XPIJn_6FHNCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2rOGDHN9LjI"
      },
      "outputs": [],
      "source": [
        "output.start_logits.shape, output.end_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " used to **`find the positions (indices) in the sequence with the highest logit`** values. Specifically,\n",
        "\n",
        "  argmax(dim=-1) is applied along the last dimension of the logits tensor, which corresponds to the different positions in the sequence."
      ],
      "metadata": {
        "id": "9AZLRtDOE4rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKcoNuuA9LjJ"
      },
      "outputs": [],
      "source": [
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WENeNC519LjJ"
      },
      "source": [
        "This will work great in a lot of cases, but what if this prediction gives us something impossible: the start position could be greater than the end position, or point to a span of text in the question instead of the answer. In that case, we might want to look at the second best prediction to see if it gives a possible answer and select that instead.\n",
        "\n",
        "However, picking the second best answer is not as easy as picking the best one: is it the second best index in the start logits with the best index in the end logits? Or the best index in the start logits with the second best index in the end logits? And if that second best answer is not possible either, it gets even trickier for the third best answer.\n",
        "\n",
        "\n",
        "To classify our answers, we will use the score obtained by adding the start and end logits. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call `n_best_size`. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one. Here is how we would do this on the first feature in the batch:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if n_best_size is set to 20, the post-processing step will **`consider the top 20 candidate answer`** spans based on their scores\n",
        "\n",
        "\n",
        "**`Candidate Answer Spans`**\n",
        "\n",
        "These are possible answers that the model identifies based on the logits (scores) assigned to different positions in the input context."
      ],
      "metadata": {
        "id": "dfyYlXTQF_9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X0Hyqur9LjJ"
      },
      "outputs": [],
      "source": [
        "n_best_size = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`1-`**Convert start_logits and end_logits to NumPy arrays for easier manipulation.\n",
        "\n",
        "**`2-`**Get indices of the top candidates:\n",
        "\n",
        "**`3-`**Generate valid answers: It iterates through combinations of start and end indices, checking if the start index is less than or equal to the end index. This ensures a valid answer span\n",
        "\n",
        "check **`ensures that the start index of a candidate answer comes before or at the same position as the end index`**\n",
        "\n"
      ],
      "metadata": {
        "id": "e0-hhfvHIpG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLnmVJDh9LjJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#Convert start_logits and end_logits to NumPy arrays for easier manipulation.\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "#Get indices of the top candidates:\n",
        "#it gets the indices of the top candidates in descending order.\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "\n",
        "#Generate valid answers: It iterates through combinations of start and end indices,\n",
        "#checking if the start index is less than or equal to the end index. This ensures a valid answer span\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
        "                }\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOCDW7Vg9LjJ"
      },
      "source": [
        "And then we can sort the `valid_answers` according to their `score` and only keep the best one. The only point left is how to check a given span is inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
        "- the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "- the offset mapping that will give us a map from token indices to character positions in the context.\n",
        "\n",
        "That's why we will re-process the validation set with the following function, slightly different from `prepare_train_features`:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`this function prepares validation features by tokenizing the questions and contexts, handling overflow, and maintaining mappings between features and examples. It ensures that the tokenized validation data is appropriately formatted for evaluation during the fine-tuning process.`**"
      ],
      "metadata": {
        "id": "61rVPVpNM5qc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UJ6jRaa9LjJ"
      },
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "\n",
        "    #Whitespace Removal:ensures consistent formatting and helps with tokenization.\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAghGd7K9LjK"
      },
      "source": [
        "And like before, we can apply that function to our validation set easily:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`prepare_validation_features:`**\n",
        "\n",
        "The function used to process each example in the validation dataset.\n",
        "\n",
        "**`batched=True: `**\n",
        "\n",
        "Indicates that the function should be applied in a batched manner, which can **improve efficiency during processing**\n",
        "\n",
        "**`remove_columns=datasets[\"validation\"].column_names: `**\n",
        "\n",
        "**Removes the specified columns** from the processed features. This is done to **clean up unnecessary information and reduce memory usage.**\n",
        "\n",
        "**`validation_features`**:\n",
        "\n",
        "Stores the processed validation features."
      ],
      "metadata": {
        "id": "aEnzRgMMOWmt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idPeNLgI9LjK"
      },
      "outputs": [],
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`validation_features,`** **is a tokenized and formatted version of the validation dataset, ready to be used for model evaluation**. It contains information such as input IDs, attention masks, offset mappings, and example IDs. During evaluation, this preprocessed dataset is fed into the fine-tuned model to obtain predictions, which can then be compared with the ground truth to assess the model's performance on the validation set."
      ],
      "metadata": {
        "id": "EY_CgVhdP3fH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx15RTeD9LjK"
      },
      "source": [
        "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jREGk5y9LjK"
      },
      "outputs": [],
      "source": [
        "raw_predictions = trainer.predict(validation_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3MuHNDg9LjK"
      },
      "source": [
        "The `Trainer` *hides* the columns that are not used by the model (here `example_id` and `offset_mapping` which we will need for our post-processing), so we set them back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XITiubDy9LjK"
      },
      "outputs": [],
      "source": [
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFvx_DD19LjK"
      },
      "source": [
        "We can now refine the test we had before: since we set `None` in the offset mappings when it corresponds to a part of the question, it's easy to check if an answer is fully inside the context. We also eliminate very long answers from our considerations (with an hyper-parameter we can tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXTqwuRR9LjL"
      },
      "outputs": [],
      "source": [
        "max_answer_length = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Offset Mappings`**\n",
        "\n",
        "ex\n",
        "running\" in the tokenized input corresponds to the characters 10 to 16 in the original text, the offset mapping for that token would be [(10, 16)]."
      ],
      "metadata": {
        "id": "Av_YIhf8Thdg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrtzzCsz9LjL"
      },
      "outputs": [],
      "source": [
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "\n",
        "#It fetches the original context (the passage of text) from the validation dataset.\n",
        "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
        "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
        "# an example index\n",
        "context = datasets[\"validation\"][0][\"context\"]\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "#Filtering and Forming Valid Answers:\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "        # to part of the input_ids that are not in the context.\n",
        "        if (\n",
        "            start_index >= len(offset_mapping)\n",
        "            or end_index >= len(offset_mapping)\n",
        "            or offset_mapping[start_index] is None\n",
        "            or offset_mapping[end_index] is None\n",
        "        ):\n",
        "            continue\n",
        "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "            continue\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            start_char = offset_mapping[start_index][0]\n",
        "            end_char = offset_mapping[end_index][1]\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": context[start_char: end_char]\n",
        "                }\n",
        "            )\n",
        "#Selecting Top Answers:\n",
        "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "valid_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIFQdxEw9LjL"
      },
      "source": [
        "We can compare to the actual ground-truth answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it includes information about the text of the answer and the character position where the answer starts in the original context."
      ],
      "metadata": {
        "id": "K9uQq9b8VUXh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln3khiY39LjM"
      },
      "outputs": [],
      "source": [
        "datasets[\"validation\"][0][\"answers\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX2iQ9Av9LjM"
      },
      "source": [
        "Our model picked the right as the most likely answer!\n",
        "\n",
        "As we mentioned in the code above, this was easy on the first feature because we knew it comes from the first example. For the other features, we will need a map between examples and their corresponding features. Also, since one example can give several features, we will need to gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`example_id_to_index:`**\n",
        "\n",
        "It creates a dictionary that maps example IDs to their corresponding index in the validation dataset.\n",
        "\n",
        "**`features_per_example:`**\n",
        "\n",
        "It is a defaultdict that collects lists of feature indices for each example."
      ],
      "metadata": {
        "id": "7We9PQbyWA_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of organizing the features in this way is to **facilitate the grouping of features based on the example** to which they belong"
      ],
      "metadata": {
        "id": "vpJNWnv0WO9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyphHcEV9LjM"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "examples = datasets[\"validation\"]\n",
        "features = validation_features\n",
        "\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "features_per_example = collections.defaultdict(list)\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqNQTIa9LjM"
      },
      "source": [
        "We're almost ready for our post-processing function. The last bit to deal with is the impossible answer (when `squad_v2 = True`). The code above only keeps answers that are inside the context, we need to also grab the score for the impossible answer (which has start and end indices corresponding to the index of the CLS token). When one example gives several features, we have to predict the impossible answer when all the features give a high score to the impossible answer (since one feature could predict the impossible answer just because the answer isn't in the part of the context it has access too), which is why the score of the impossible answer for one example is the *minimum* of the scores for the impossible answer in each feature generated by the example.\n",
        "\n",
        "We then predict the impossible answer when that score is greater than the score of the best non-impossible answer. All combined together, this gives us this post-processing function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n84H914x9LjM"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        if not squad_v2:\n",
        "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "        else:\n",
        "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "            predictions[example[\"id\"]] = answer\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xjXXOYa9LjM"
      },
      "source": [
        "And we can apply our post-processing function to our raw predictions:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply validation"
      ],
      "metadata": {
        "id": "FIqnR-Pvgf31"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59oaNQTb9LjN"
      },
      "outputs": [],
      "source": [
        "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWV56RmV9LjN"
      },
      "source": [
        "Then we can load the metric from the datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVNcda3C9LjN"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoOgdklE9LjN"
      },
      "source": [
        "Then we can call compute on it. We just need to format predictions and labels a bit as it expects a list of dictionaries and not one big dictionary. In the case of squad_v2, we also have to set a `no_answer_probability` argument (which we set to 0.0 here as we have already set the answer to empty if we picked it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK-0iMID9LjN"
      },
      "outputs": [],
      "source": [
        "if squad_v2:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
        "else:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`exact match`** score of 73.12% indicates **`the percentage of predictions that exactly match the ground truth answers`**"
      ],
      "metadata": {
        "id": "O4hSSH_Kfe2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = {'exact_match': 76.81173131504258, 'f1': 85.18108051522785}\n",
        "\n",
        "# Plotting the metrics\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['blue', 'green'])\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Exact Match and F1 Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HeeMAVIUW0bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "CvuPNMtmTIFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# > **TESTING**\n",
        "\n"
      ],
      "metadata": {
        "id": "uthy4jo-Y6oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your training code here\n",
        "\n",
        "# Save the fine-tuned model\n",
        "save_path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n"
      ],
      "metadata": {
        "id": "M8D_nD4TAIjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load the fine-tuned DistilBERT model and tokenizer\n",
        "fine_tuned_model = AutoModelForQuestionAnswering.from_pretrained(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\")\n"
      ],
      "metadata": {
        "id": "9R0DN0iQASkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade\n"
      ],
      "metadata": {
        "id": "XPQSNY2PBqt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load the fine-tuned DistilBERT model and tokenizer\n",
        "model_name = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "def generate_answer_distilbert_squad(row):\n",
        "    question = row[\"question\"]\n",
        "    context = row[\"context\"]\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=384,  # Adjust the max length as per your fine-tuning\n",
        "        padding=\"max_length\",\n",
        "        truncation=\"only_second\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bs, q_length, dim = encoding[\"input_ids\"].size() if len(encoding[\"input_ids\"].size()) == 3 else encoding[\"input_ids\"].size(0), encoding[\"input_ids\"].size(-2), encoding[\"input_ids\"].size(-1)\n",
        "\n",
        "        start_logits = output.start_logits.argmax(dim=-1)\n",
        "        end_logits = output.end_logits.argmax(dim=-1)\n",
        "\n",
        "        start_index = torch.argmax(start_logits).item()\n",
        "        end_index = torch.argmax(end_logits).item()\n",
        "\n",
        "    answer = tokenizer.decode(encoding[\"input_ids\"][0][start_index:end_index+1], skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        \"context\": context,\n",
        "        \"question\": question,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "# Example usage with a row from your SQuAD dataset\n",
        "sample_row = {\n",
        "    \"question\": \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
        "    \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend 'Venite Ad Me Omnes'. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\",\n",
        "    \"id\": \"5733be284776f41900661182\",\n",
        "}\n",
        "\n",
        "result = generate_answer_distilbert_squad(sample_row)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "Y33H8tCj_jdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n"
      ],
      "metadata": {
        "id": "Ws9tGWwC0cii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "_axbKKxt0eTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_example = \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\"...\"\n",
        "\n",
        "question_example = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "\n",
        "answer_example = get_answer(context_example, question_example)\n",
        "\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Answer: {answer_example}\")\n"
      ],
      "metadata": {
        "id": "SqyZuCKV0eQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "QdvJjXib2foD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Create a function for question answering\n",
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n",
        "\n",
        "# Test the function with a context and question\n",
        "context_example = \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\n",
        "question_example = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "answer_example = get_answer(context_example, question_example)\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Answer: {answer_example}\")\n"
      ],
      "metadata": {
        "id": "FG_MxCNlYPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Answer: {answer_example}\")"
      ],
      "metadata": {
        "id": "fOyIYizXT14x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Create a function for question answering\n",
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n",
        "\n",
        "# New context and question\n",
        "new_context = \"Paris, the capital of France, is renowned for its architecture, art, and rich history. The Eiffel Tower, an iconic landmark, stands tall on the Champ de Mars, offering breathtaking views of the city. The Louvre Museum, home to thousands of works of art, including the Mona Lisa, attracts millions of visitors each year. Paris is known for its charming streets, sidewalk cafes, and vibrant cultural scene.\"\n",
        "\n",
        "new_question = \"What is the most famous landmark in Paris?\"\n",
        "new_answer = get_answer(new_context, new_question)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Question: {new_question}\")\n",
        "print(f\"Answer: {new_answer}\")\n"
      ],
      "metadata": {
        "id": "oKvQ4d2PHW1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Create a function for question answering\n",
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n",
        "\n",
        "# New context and question\n",
        "#new_context = \"Paris, the capital of France, is renowned for its architecture, art, and rich history. The Eiffel Tower, an iconic landmark, stands tall on the Champ de Mars, offering breathtaking views of the city. The Louvre Museum, home to thousands of works of art, including the Mona Lisa, attracts millions of visitors each year. Paris is known for its charming streets, sidewalk cafes, and vibrant cultural scene.\"\n",
        "\n",
        "new_question = \"During whose rule was the use of Old Akkadian at its peak?\"\n",
        "new_answer = get_answer(new_context, new_question)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Question: {new_question}\")\n",
        "print(f\"Answer: {new_answer}\")\n"
      ],
      "metadata": {
        "id": "MJg2UiVeg6uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "\n",
        "# Iterate through SQuAD examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "1IXOBcv5koUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load SQuAD 2.0 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Print the first example in the training set\n",
        "print(squad_dataset[\"train\"][0])\n"
      ],
      "metadata": {
        "id": "gq2FjYeIrD0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(squad_dataset[\"train\"][i])\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Add a separator for better readability"
      ],
      "metadata": {
        "id": "si-PYJkJrZpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Print 10 random examples from SQuAD v2 dataset\n",
        "for _ in range(10):\n",
        "    random_index = random.randint(0, len(squad_dataset[\"train\"]) - 1)\n",
        "    example = squad_dataset[\"train\"][random_index]\n",
        "\n",
        "    print(example)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Add a separator for better readability\n"
      ],
      "metadata": {
        "id": "_-OhGJyD0OiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"Who managed the Destiny's Child group?\"\n",
        "\n",
        "# Iterate through SQuAD v2 examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "sOxElA2asPyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"What would happen without a proper five-year plan?\"\n",
        "\n",
        "# Iterate through SQuAD v2 examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "84Zz8lkrulEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"Who's concept of duration was left  behind for a for more concrete frame's of references? \"\n",
        "\n",
        "# Iterate through SQuAD v2 examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "33vK2WfaodjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers scikit-learn\n"
      ],
      "metadata": {
        "id": "NcSvTCDT7uBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question (rephrased)\n",
        "question_example_rephrased = \"Whose concept of duration were abandoned for moore concrete frames of reference?\"\n",
        "\n",
        "# Compute embeddings for all questions in the dataset\n",
        "embedder = SentenceTransformer(\"paraphrase-distilroberta-base-v1\")\n",
        "question_embeddings = embedder.encode(squad_dataset[\"train\"][\"question\"])\n",
        "\n",
        "# Compute the embedding for the rephrased question\n",
        "rephrased_embedding = embedder.encode([question_example_rephrased])[0]\n",
        "\n",
        "# Measure cosine similarity between the rephrased question and all questions in the dataset\n",
        "similarities = cosine_similarity([rephrased_embedding], question_embeddings)[0]\n",
        "\n",
        "# Find the most similar question\n",
        "most_similar_index = similarities.argmax()\n",
        "most_similar_question = squad_dataset[\"train\"][\"question\"][most_similar_index]\n",
        "context = squad_dataset[\"train\"][\"context\"][most_similar_index]\n",
        "\n",
        "# Use the most similar question's context to generate an answer\n",
        "inputs = tokenizer(question_example_rephrased, context, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "start_idx = start_logits.argmax(-1).item()\n",
        "end_idx = end_logits.argmax(-1).item() + 1\n",
        "answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "# Print the results\n",
        "print(f\"Rephrased Question: {question_example_rephrased}\")\n",
        "print(f\"Most Similar Question: {most_similar_question}\")\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "Bm2LmMSA7znk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question (rephrased)\n",
        "question_example_rephrased = \"On the banks of which river did the Avars originally establish their base?\"\n",
        "\n",
        "# Compute embeddings for all questions in the dataset\n",
        "embedder = SentenceTransformer(\"paraphrase-distilroberta-base-v1\")\n",
        "question_embeddings = embedder.encode(squad_dataset[\"train\"][\"question\"])\n",
        "\n",
        "# Compute the embedding for the rephrased question\n",
        "rephrased_embedding = embedder.encode([question_example_rephrased])[0]\n",
        "\n",
        "# Measure cosine similarity between the rephrased question and all questions in the dataset\n",
        "#Cosine similarity is a measure of similarity between two vectors\n",
        "similarities = cosine_similarity([rephrased_embedding], question_embeddings)[0]\n",
        "\n",
        "# Find the most similar question\n",
        "#return maximum value\n",
        "most_similar_index = similarities.argmax()\n",
        "most_similar_question = squad_dataset[\"train\"][\"question\"][most_similar_index]\n",
        "context = squad_dataset[\"train\"][\"context\"][most_similar_index]\n",
        "\n",
        "#Find the index of the question in the SQuAD dataset that has the highest cosine similarity with the rephrased question.\n",
        "\n",
        "\n",
        "\n",
        "# Use the most similar question's context to generate an answer\n",
        "inputs = tokenizer(question_example_rephrased, context, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "start_idx = start_logits.argmax(-1).item()\n",
        "end_idx = end_logits.argmax(-1).item() + 1\n",
        "answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "# Print the results\n",
        "print(f\"Rephrased Question: {question_example_rephrased}\")\n",
        "print(f\"Most Similar Question: {most_similar_question}\")\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "4iLvqcq8I_yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "nr3ELN3LRfw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Example question from SQuAD v1 dataset\n",
        "question_example = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "\n",
        "# Generate answer directly from the model\n",
        "inputs = tokenizer(question_example, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "start_idx = start_logits.argmax(-1).item()\n",
        "end_idx = end_logits.argmax(-1).item() + 1\n",
        "answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "# Print the results\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "sAeB86apBqME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You can now upload the result of the training to the Hub, just execute this instruction:**"
      ],
      "metadata": {
        "id": "rci2EnBLLpRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Check if the model exists\n",
        "model_name = \"distilbert-base-uncased-finetuned-squad\"\n",
        "try:\n",
        "    pipeline(model=model_name, tokenizer=model_name)\n",
        "    print(f\"The model {model_name} exists and is accessible.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "4LKgtmZLv-7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-squad\"\n",
        "\n",
        "try:\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    print(\"Config file (config.json) exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"Config file (config.json) is missing. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "4TqiVM9qx4jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "config_content = {\n",
        "    \"architectures\": [\"DistilBertForQuestionAnswering\"],\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"num_labels\": 2,\n",
        "    \"id2label\": {\"0\": \"LABEL_0\", \"1\": \"LABEL_1\"},\n",
        "    \"label2id\": {\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
        "}\n",
        "\n",
        "# Use the current working directory as the output directory\n",
        "output_directory = os.getcwd()\n",
        "config_path = os.path.join(output_directory, \"config.json\")\n",
        "\n",
        "# Write the content to the config.json file\n",
        "with open(config_path, \"w\") as config_file:\n",
        "    json.dump(config_content, config_file, indent=4)\n",
        "\n",
        "print(f\"Config file (config.json) has been created at: {config_path}\")\n"
      ],
      "metadata": {
        "id": "SATSBUe41Ojf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNGA3y8K9LjN"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87WK7UY99LjN"
      },
      "source": [
        "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"sgugger/my-awesome-model\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9whIBKlr9LjO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "rEJBSTyZIrIb",
        "whPRbBNbIrIl",
        "n9qywopnIrJH",
        "545PP3o8IrJV",
        "_QqGzIb89LjI",
        "uthy4jo-Y6oX",
        "rci2EnBLLpRb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "266152b720ce4abaa7178cdb553f379e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9baea6eae06c4eebb17f5c03545d125c",
              "IPY_MODEL_0c5c053104b748199540d81c55a04bda",
              "IPY_MODEL_17238d55309a404b852cc758f663cd9d",
              "IPY_MODEL_e1b7e337da704b8497aa71032c303f6a",
              "IPY_MODEL_98cf8d173a8f4239abd319b5dfbb3b27"
            ],
            "layout": "IPY_MODEL_cd5a628644bd485a8aea850ea07ddf82"
          }
        },
        "9baea6eae06c4eebb17f5c03545d125c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc84ec8b69114b918fd4acb5b6e2d99d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1ce01442ccb2498da4cf2f0c2cdc04cb",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0c5c053104b748199540d81c55a04bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_21cefd284b494db894cdfe3278c0d0d4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af424ce0e72c497aa5ee6140e86cba89",
            "value": ""
          }
        },
        "17238d55309a404b852cc758f663cd9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2fb70996ce82416193a2ec3c97cf8b21",
            "style": "IPY_MODEL_338027b315e149a5bca2817ed0d2ab13",
            "value": true
          }
        },
        "e1b7e337da704b8497aa71032c303f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a56d55e380bf4a4987f987028bb8ccdd",
            "style": "IPY_MODEL_4061a372dbb8436581891712a930f969",
            "tooltip": ""
          }
        },
        "98cf8d173a8f4239abd319b5dfbb3b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221bc73560e64439a77864c3aaeaea52",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7527aba3fe94471e8eaa313ff2ab9025",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "cd5a628644bd485a8aea850ea07ddf82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "bc84ec8b69114b918fd4acb5b6e2d99d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce01442ccb2498da4cf2f0c2cdc04cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21cefd284b494db894cdfe3278c0d0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af424ce0e72c497aa5ee6140e86cba89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fb70996ce82416193a2ec3c97cf8b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "338027b315e149a5bca2817ed0d2ab13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a56d55e380bf4a4987f987028bb8ccdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4061a372dbb8436581891712a930f969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "221bc73560e64439a77864c3aaeaea52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7527aba3fe94471e8eaa313ff2ab9025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ffdddf404e7471f90fd8a0d27f88047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a673386a101f4f10bab319cd3e2d756e",
              "IPY_MODEL_2a3c4cd174834cc2afaf4d01dc9f077c",
              "IPY_MODEL_ac6ee5e07bca4c81bfe32c6515f823ce"
            ],
            "layout": "IPY_MODEL_fb0ce3d2af154caf9127410fb343cd8b"
          }
        },
        "a673386a101f4f10bab319cd3e2d756e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2c2688340642d9a9a8e3d26d3f4f76",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_10edf8f29bdf4ab48737e1e8afed1d3a",
            "value": "Map: 100%"
          }
        },
        "2a3c4cd174834cc2afaf4d01dc9f077c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954c31c0daac44d3b60d2bbfaa69ea35",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_842644a9ceb24d668fee97ae3fc5c6e2",
            "value": 10570
          }
        },
        "ac6ee5e07bca4c81bfe32c6515f823ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c65fdced464f619e737ab25091a944",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_83128949f13648f58e8b0a58d822fdef",
            "value": " 10570/10570 [00:08&lt;00:00, 1376.22 examples/s]"
          }
        },
        "fb0ce3d2af154caf9127410fb343cd8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb2c2688340642d9a9a8e3d26d3f4f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10edf8f29bdf4ab48737e1e8afed1d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954c31c0daac44d3b60d2bbfaa69ea35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "842644a9ceb24d668fee97ae3fc5c6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4c65fdced464f619e737ab25091a944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83128949f13648f58e8b0a58d822fdef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}