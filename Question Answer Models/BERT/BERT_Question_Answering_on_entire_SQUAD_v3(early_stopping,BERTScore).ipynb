{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omar-Aliii/AI-AGENT/blob/main/Question%20Answer%20Models/BERT/BERT_Question_Answering_on_entire_SQUAD_v3(early_stopping%2CBERTScore).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ü§ó Transformers and ü§ó Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers: This library by Hugging Face provides a collection of pre-trained models for Natural Language Processing (NLP) tasks, including BERT, GPT, RoBERTa, and more. It also includes tools for training your own models and fine-tuning existing ones."
      ],
      "metadata": {
        "id": "FCQbcIIEkZ07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "2eab7082-98c1-418f-a79b-a82722fbaca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET1wfaBF9Li2"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import pipeline\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, EarlyStoppingCallback\n"
      ],
      "metadata": {
        "id": "mU7tXkopmzUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/deepset-ai/haystack.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRBCqgU7-4s5",
        "outputId": "eb790fac-9cf8-433e-ec6a-c1e07eb16838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/deepset-ai/haystack.git\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-req-build-mkolhl0f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git /tmp/pip-req-build-mkolhl0f\n",
            "  Resolved https://github.com/deepset-ai/haystack.git to commit acf4cd502fd67c50c3bbbd3eb580738ff041c28f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boilerpy3 (from haystack-ai==2.0.0b5)\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Collecting haystack-bm25 (from haystack-ai==2.0.0b5)\n",
            "  Downloading haystack_bm25-1.0.2-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (3.1.3)\n",
            "Collecting lazy-imports (from haystack-ai==2.0.0b5)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (3.2.1)\n",
            "Collecting openai>=1.1.0 (from haystack-ai==2.0.0b5)\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (1.5.3)\n",
            "Collecting posthog (from haystack-ai==2.0.0b5)\n",
            "  Downloading posthog-3.3.3-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (6.0.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.0.0b5) (4.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.0.0b5) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai==2.0.0b5) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai==2.0.0b5)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.0.0b5) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.0.0b5) (1.3.0)\n",
            "Collecting typing-extensions (from haystack-ai==2.0.0b5)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-bm25->haystack-ai==2.0.0b5) (1.23.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai==2.0.0b5) (2.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai==2.0.0b5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai==2.0.0b5) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai==2.0.0b5) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai==2.0.0b5) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai==2.0.0b5)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai==2.0.0b5)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai==2.0.0b5) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai==2.0.0b5) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai==2.0.0b5) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai==2.0.0b5)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai==2.0.0b5)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai==2.0.0b5) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai==2.0.0b5) (2.0.7)\n",
            "Building wheels for collected packages: haystack-ai\n",
            "  Building wheel for haystack-ai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for haystack-ai: filename=haystack_ai-2.0.0b5-py3-none-any.whl size=233865 sha256=1d533f72ac4c2eb4675ff3165b45c91e0c5b7dc345b345f856c3a51bf174af32\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xkf7we4h/wheels/e7/f8/df/e7c3a1fea47ea9a9db18c056e698a82658e0fe5814c5608165\n",
            "Successfully built haystack-ai\n",
            "Installing collected packages: monotonic, typing-extensions, lazy-imports, haystack-bm25, h11, boilerpy3, backoff, posthog, httpcore, httpx, openai, haystack-ai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 boilerpy3-1.0.7 h11-0.14.0 haystack-ai-2.0.0b5 haystack-bm25-1.0.2 httpcore-1.0.2 httpx-0.26.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.10.0 posthog-3.3.3 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n"
      ],
      "metadata": {
        "id": "za9raOr8m16q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU1cyFC69Li2",
        "outputId": "223f50de-1016-482e-ec4d-9e50efd19b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "7bd00e1b2f51422fa5f78a35eef57398",
            "8b2c077d2bd94dfbbb4145883c1ed910",
            "adafffc9691449199c756c4d05d3d39a",
            "f5cd929b520d4836a2ced29e2b7848a8",
            "a55d8b7e8e054239a02994bc376baa01",
            "3334b4f86aec4a9cb0563e1351ba21f7",
            "295ec0ddab7c4da3a9a1a3386240073f",
            "5a7d73179da94df2a3213c6dba390c5b",
            "5bc72a6d8908461ea33150e6a66ebb85",
            "bd5fc16ccfd3428daf3961a0a787fa04",
            "65b14a6fef2b438790f0290e6d35896c",
            "46352aced18f44d2832a2435ca576c92",
            "89570a66c82544be86906fd3981babb1",
            "2cb2b417cb844ff88d02fd0a9190d155",
            "21b820301a0b43d18cdbea0fe75ccf6d",
            "fe7326f283cc4eed869da533f5d523a4",
            "f8bd4645964b41e9872610dbe10440c3"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bd00e1b2f51422fa5f78a35eef57398"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbaL0CNX9Li3"
      },
      "source": [
        "Then you need to install Git-LFS. Uncomment the following instructions:\n",
        "\n",
        "Git LFS is an extension to Git that allows managing large files efficiently. It replaces large files in your repository with text pointers inside Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh-zKawM9Li3",
        "outputId": "caa758aa-fc27-444a-86b4-32c47778cc3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBUkOTMt9Li4"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-QMnsSV9Li4",
        "outputId": "d800efd9-0e76-4b82-9172-c8957aee0fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/question-answering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh8hYpXc9Li5"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import send_example_telemetry\n",
        "#send information to hugging face\n",
        "send_example_telemetry(\"question_answering_notebook\", framework=\"pytorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "send_example_telemetry function you're using is part of the Hugging Face Transformers library, and **it is used to send telemetry data about model usage to Hugging Face**. This helps them gather information about how their models are being used in the community."
      ],
      "metadata": {
        "id": "Q97C5zt-lrgX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a question-answering task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
        "# answers are allowed or not).\n",
        "\n",
        "squad_v2 = False\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "squad_v2: This flag indicates whether you're working with SQuAD version 2 (True) or version 1 (False). SQuAD v2 introduced questions where the answer is not present in the provided passage, making it a more challenging dataset. If squad_v2 is False, it suggests that you're working with SQuAD v1, where every question has a corresponding answer in the passage.\n",
        "\n",
        "**model_checkpoint:** This variable specifies the model checkpoint or pre-trained model you want to use for fine-tuning. In this case, it's set to \"distilbert-base-uncased,\" which is a smaller and faster version of BERT (Bidirectional Encoder Representations from Transformers) pre-trained on uncased text. You can replace this with other model checkpoints available in the Hugging Face model hub\n",
        "\n",
        "**batch_size**:  It determines how many examples are processed in each iteration during training.\n",
        " model will compute gradients and update its parameters based on the average loss calculated over these 16 examples\n",
        "\n",
        "  batch size often depends on various factors, including the **available memory on the GPU or CPU, the size of the dataset**"
      ],
      "metadata": {
        "id": "fkSEcR9gm5zt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`load_dataset function`**:\n",
        "\n",
        "This function is used to load datasets from the Hugging Face datasets library.\n",
        "\n",
        "**`load_metric function:`**\n",
        "\n",
        "This function is used to load evaluation metrics from the datasets library.\n",
        "\n",
        "\n",
        "these metrics provide a way to quantify how well a model is performing in terms of its predictions compared to the ground truth or correct answers."
      ],
      "metadata": {
        "id": "iN9s2bSVng8m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "For our example here, we'll use the [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/). The notebook should work with any question answering dataset provided by the ü§ó Datasets library. If you're using your own dataset defined from a JSON or csv file (see the [Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) on how to load them), it might need some adjustments in the names of the columns used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_AY1ATSIrIq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "3d233451031f47568977398a28084f32",
            "4c2868c3680146218c02d53cf86c1ea7",
            "e84df80b8c7d4b32a3aae8171b2f395c",
            "8461d79141624c848273ac5b4274ee12",
            "26b6a33026d741c1a5cb039e619ddc2a",
            "d1a94874796048418100d4bafdee93e2",
            "3893f773067246019513c4a2fbd22647",
            "4c465156f43543438a5e3c30ae66ba49",
            "d8d51741c6ff48d6b275b258b22873df",
            "87e498e9d6f946e1abf85f1f53e4592b",
            "33b69afd955a4c098e91b07d03935987",
            "76eeb23a15784f148fbf2386e9bda6b4",
            "6bbfd8c093a14a1f85af59bf59d3098c",
            "49f44fcdfc6b4e1b834f255ff2aa0c72",
            "3922c3ee6fe244a0abf6d97523fd472b",
            "7a7198148d7d41279c7b8110ad3d3f08",
            "c09f24d4d9194569a014ebbee203da9f",
            "1ff2a96807a045448ff16640f5abdb8e",
            "ff1a188d072841b88756d7e4b6da1267",
            "9ce772b4096c49978dac88bf314e43fb",
            "81868ae800154ce082e0e044148bb810",
            "c5261fb25c7c4b349d67572ec808ba5d",
            "8683a37b81af47e9af1bcf90b5fca885",
            "5c57e8a119de4343af0b97e7c1311718",
            "42d7989a4d974413a1a22155376ca1bc",
            "835869141a9d4c8d9506d0234ab21fca",
            "5b2918bb8d404a65967484a9db1359d9",
            "7584bab1611e4cc0a1a45aa4660ad617",
            "29fb41a96fbe410f9630a46fda015419",
            "c6684f59fa31451b89a2ef4da4a1b503",
            "d6afb9247d924be09ebfefadab0de3cf",
            "d5756765f3b94af0b7cae88645722cae",
            "ce784daeb0ba454c943a6213b5ecadc4",
            "5ba81c93e9f24168aff805d25e3777cb",
            "698f7b9e320146229e4582d374d1dbc0",
            "44e58997083348a5845f6907c1285d8f",
            "6199c87fd1684b70b75635e989509f3b",
            "a2c2ec61c6ca4149be4b3bd1aae1f4ba",
            "ca34dec26eaf492f93fe8d4da68691c9",
            "0ba1b25268e24aab8d372f8f1d8e44c0",
            "688375edee6d4b2cac827834efa316a7",
            "a2c4d3a763584e8d99dbde67072f4c5b",
            "b0f0c560e1034803b7aa33fb341e53bb",
            "854715dfeda945f7bb33cf5110d939cb",
            "915c9a826c2742f8939b0b522331f55e",
            "092304d802ac4b9aba4dd6dfcbaee109",
            "a96680260d3f42db9bcb58958d808c27",
            "8ade84e485e64c33a110b3fa85651da8",
            "a7249be0aba94974b1016af4cc9298ef",
            "9a6013fbfd434ee690362d58b231223c",
            "f81a5f6b500c41c38f84ae5ee71aa5b5",
            "8c9f461ca1814681bafe115111d0e98b",
            "d8c5cfcc725e45ee888b20a615b038ad",
            "e21e5b472c4d406f832e0916d8caf287",
            "3548db6299014ed49e074f6621dfa74e"
          ]
        },
        "outputId": "52d7dcf0-0477-4425-b3df-e0a2de36ba6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/7.83k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d233451031f47568977398a28084f32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76eeb23a15784f148fbf2386e9bda6b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8683a37b81af47e9af1bcf90b5fca885"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ba81c93e9f24168aff805d25e3777cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "915c9a826c2742f8939b0b522331f55e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWiVUF0jIrIv",
        "outputId": "23231a92-0e2f-4384-832e-8f2b5af96e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFgwHAZe9Li9"
      },
      "source": [
        "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "datasets[\"train\"][0] is used to access the first example in the training split of your SQuAD dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6HrpprwIrIz",
        "outputId": "b7f8913d-aa89-4d14-e20f-1b000b4fadb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer_start': [515]:** the answer of the question start with character at position 515"
      ],
      "metadata": {
        "id": "cO-8CjbhrmM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "jBdFomQ-r76z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "show_random_elements that takes a dataset and displays a specified number of randomly picked examples. The function decodes labels if they are encoded (e.g., if they are represented as integer indices), making it easier to understand the content of the dataset\n",
        "\n",
        "\n",
        "**how the function works:**\n",
        "\n",
        "It randomly selects indices to pick examples from the dataset, ensuring that\n",
        "\n",
        "the same example is not picked more than once.\n",
        "\n",
        "It creates a Pandas DataFrame from the selected examples.\n",
        "\n",
        "It decodes class labels if they are represented as integer indices.\n",
        "\n",
        "It displays the DataFrame for better tabular representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "#The dataset from which random elements will be displayed.\n",
        "# The number of random examples to display (default is 10).\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZy5tRB_IrI7",
        "outputId": "f4964722-f878-4503-c50a-2c54ed007d32",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>573402a3d058e614000b6797</td>\n",
              "      <td>Genocide</td>\n",
              "      <td>All signatories to the CPPCG are required to prevent and punish acts of genocide, both in peace and wartime, though some barriers make this enforcement difficult. In particular, some of the signatories‚Äînamely, Bahrain, Bangladesh, India, Malaysia, the Philippines, Singapore, the United States, Vietnam, Yemen, and former Yugoslavia‚Äîsigned with the proviso that no claim of genocide could be brought against them at the International Court of Justice without their consent. Despite official protests from other signatories (notably Cyprus and Norway) on the ethics and legal standing of these reservations, the immunity from prosecution they grant has been invoked from time to time, as when the United States refused to allow a charge of genocide brought against it by former Yugoslavia following the 1999 Kosovo War.</td>\n",
              "      <td>Signatories to the CPPC are required to prevent and punish what?</td>\n",
              "      <td>{'text': ['acts of genocide'], 'answer_start': [64]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56cdf52c62d2951400fa69cb</td>\n",
              "      <td>Spectre_(2015_film)</td>\n",
              "      <td>Following filming in Mexico, and during a scheduled break, Craig was flown to New York to undergo minor surgery to fix his knee injury. It was reported that filming was not affected and he had returned to filming at Pinewood Studios as planned on 22 April.</td>\n",
              "      <td>When did Craig go back to work?</td>\n",
              "      <td>{'text': ['22 April'], 'answer_start': [247]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57313428e6313a140071cd15</td>\n",
              "      <td>Indigenous_peoples_of_the_Americas</td>\n",
              "      <td>The first indigenous group encountered by Columbus were the 250,000 Ta√≠nos of Hispaniola who represented the dominant culture in the Greater Antilles and the Bahamas. Within thirty years about 70% of the Ta√≠nos had died. They had no immunity to European diseases, so outbreaks of measles and smallpox ravaged their population. Increasing punishment of the Ta√≠nos for revolting against forced labour, despite measures put in place by the encomienda, which included religious education and protection from warring tribes, eventually led to the last great Ta√≠no rebellion.</td>\n",
              "      <td>What did the Ta√≠nos represent in the Greater Antilles and Bahamas?</td>\n",
              "      <td>{'text': ['dominant culture'], 'answer_start': [109]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>572e90ee03f98919007567b2</td>\n",
              "      <td>Elevator</td>\n",
              "      <td>Elevator doors protect riders from falling into the shaft. The most common configuration is to have two panels that meet in the middle, and slide open laterally. In a cascading telescopic configuration (potentially allowing wider entryways within limited space), the doors roll on independent tracks so that while open, they are tucked behind one another, and while closed, they form cascading layers on one side. This can be configured so that two sets of such cascading doors operate like the center opening doors described above, allowing for a very wide elevator cab. In less expensive installations the elevator can also use one large \"slab\" door: a single panel door the width of the doorway that opens to the left or right laterally. Some buildings have elevators with the single door on the shaft way, and double cascading doors on the cab.</td>\n",
              "      <td>What design allows wider entryways within limited space?</td>\n",
              "      <td>{'text': ['a cascading telescopic configuration'], 'answer_start': [165]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56e085d7231d4119001ac25c</td>\n",
              "      <td>Saint_Helena</td>\n",
              "      <td>The Governor's Cup is a yacht race between Cape Town and Saint Helena island, held every two years in December/January; the most recent event was in December 2010. In Jamestown a timed run takes place up Jacob's Ladder every year, with people coming from all over the world to take part.</td>\n",
              "      <td>What months does the Governor's cup take place?</td>\n",
              "      <td>{'text': ['December/January'], 'answer_start': [102]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>572f112fdfa6aa1500f8d5af</td>\n",
              "      <td>Elevator</td>\n",
              "      <td>The Twilight Zone Tower of Terror is the common name for a series of elevator attractions at the Disney's Hollywood Studios park in Orlando, the Disney California Adventure Park park in Anaheim, the Walt Disney Studios Park in Paris and the Tokyo DisneySea park in Tokyo. The central element of this attraction is a simulated free-fall achieved through the use of a high-speed elevator system. For safety reasons, passengers are seated and secured in their seats rather than standing. Unlike most traction elevators, the elevator car and counterweight are joined using a rail system in a continuous loop running through both the top and the bottom of the drop shaft. This allows the drive motor to pull down on the elevator car from underneath, resulting in downward acceleration greater than that of normal gravity. The high-speed drive motor is used to rapidly lift the elevator as well.</td>\n",
              "      <td>Do ride goers stand or sit?</td>\n",
              "      <td>{'text': ['passengers are seated and secured in their seats rather than standing'], 'answer_start': [414]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>57337cc94776f41900660ba8</td>\n",
              "      <td>Alfred_North_Whitehead</td>\n",
              "      <td>Overall, however, Whitehead's influence is very difficult to characterize. In English-speaking countries, his primary works are little-studied outside of Claremont and a select number of liberal graduate-level theology and philosophy programs. Outside of these circles his influence is relatively small and diffuse, and has tended to come chiefly through the work of his students and admirers rather than Whitehead himself. For instance, Whitehead was a teacher and long-time friend and collaborator of Bertrand Russell, and he also taught and supervised the dissertation of Willard Van Orman Quine, both of whom are important figures in analytic philosophy ‚Äì the dominant strain of philosophy in English-speaking countries in the 20th century. Whitehead has also had high-profile admirers in the continental tradition, such as French post-structuralist philosopher Gilles Deleuze, who once dryly remarked of Whitehead that \"he stands provisionally as the last great Anglo-American philosopher before Wittgenstein's disciples spread their misty confusion, sufficiency, and terror.\" French sociologist and anthropologist Bruno Latour even went so far as to call Whitehead \"the greatest philosopher of the 20th century.\"</td>\n",
              "      <td>Where has interest outside of those areas mainly come from?</td>\n",
              "      <td>{'text': ['through the work of his students and admirers rather'], 'answer_start': [347]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>572831a53acd2414000df6bd</td>\n",
              "      <td>PlayStation_3</td>\n",
              "      <td>The system displays the What's New screen by default instead of the [Games] menu (or [Video] menu, if a movie was inserted) when starting up. What's New has four sections: \"Our Pick\", \"Recently Played\", latest information and new content available in PlayStation Store. There are four kinds of content the What's New screen displays and links to, on the sections. \"Recently Played\" displays the user's recently played games and online services only, whereas, the other sections can contain website links, links to play videos and access to selected sections of the PlayStation Store.</td>\n",
              "      <td>Other than the Video default screen for movies, what menu would the PS3 default to before What's New?</td>\n",
              "      <td>{'text': ['Games'], 'answer_start': [69]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5725c0c789a1e219009abdec</td>\n",
              "      <td>Buckingham_Palace</td>\n",
              "      <td>The original early 19th-century interior designs, many of which survive, include widespread use of brightly coloured scagliola and blue and pink lapis, on the advice of Sir Charles Long. King Edward VII oversaw a partial redecoration in a Belle √âpoque cream and gold colour scheme. Many smaller reception rooms are furnished in the Chinese regency style with furniture and fittings brought from the Royal Pavilion at Brighton and from Carlton House. The palace has 775 rooms, and the garden is the largest private garden in London. The state rooms, used for official and state entertaining, are open to the public each year for most of August and September, and on selected days in winter and spring.</td>\n",
              "      <td>What house were many of the furniture and fittings brought from?</td>\n",
              "      <td>{'text': ['Carlton House'], 'answer_start': [435]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>56e72ec800c9c71400d76eeb</td>\n",
              "      <td>Daylight_saving_time</td>\n",
              "      <td>In the case of the United States where a one-hour shift occurs at 02:00 local time, in spring the clock jumps forward from the last moment of 01:59 standard time to 03:00 DST and that day has 23 hours, whereas in autumn the clock jumps backward from the last moment of 01:59 DST to 01:00 standard time, repeating that hour, and that day has 25 hours. A digital display of local time does not read 02:00 exactly at the shift to summer time, but instead jumps from 01:59:59.9 forward to 03:00:00.0.</td>\n",
              "      <td>Daylight Saving Time is sometimes called summer time, but the clocks are actually moved forward in which season?</td>\n",
              "      <td>{'text': ['spring'], 'answer_start': [87]}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Access the original dataset file\n",
        "\n",
        "train_dataset_path = squad_dataset['train'].cache_files[0] if 'train' in squad_dataset else None\n",
        "print(\"Path to the original SQuAD training dataset file:\", train_dataset_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "19mdxj3pI0O4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680064c6-ba9e-4b16-9ba9-5e7862e81449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to the original SQuAD training dataset file: {'filename': '/root/.cache/huggingface/datasets/squad/plain_text/0.0.0/0a3a8b7b57e8578ec40b2d2bb4c75aca1a6d6ce1/squad-train.arrow'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Tokenization is the process of breaking down a piece of text into individual tokens (words, subwords, or characters) that the model can understand and process.\n",
        "\n",
        "\n",
        "\n",
        "tokenizer is a crucial component in natural language processing (NLP) that breaks down a piece of text into smaller units called tokens.\n",
        "\n",
        "\n",
        "Tokenizer's Role:\n",
        "\n",
        "Input: Raw text.\n",
        "\n",
        "Output: List of tokens.\n",
        "\n",
        "Purpose: To convert variable-length sequences of characters (words or subwords) into a fixed-length sequence of tokens that can be fed into a model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`AutoTokenizer`**\n",
        "\n",
        " to load the appropriate tokenizer for a given pre-trained language model"
      ],
      "metadata": {
        "id": "g4Nj5ZVRiF3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "60b026e48c7c4d50982b0e8b05561511",
            "3c6da572ef104a2db7720ee78e2cf1f0",
            "475c460e52e44aad9dc30e65c3012bfc",
            "ecf72ba15acc4cbe9a777f5d21044598",
            "1ca6e16388c340f9b31c4c3c3a257b0d",
            "05f16b669b1447c7a88c4e2b6f713fd4",
            "09987b8065f043388f9704cbdced823a",
            "4ea716730fc0433294892d7e18bf6b3e",
            "c6ebbdfbe91e440487b269264038f528",
            "aca8d6b845ac4c0f8956dde53dcb06aa",
            "8713156648574a6e99cc99479a950320",
            "683f7b2f3bba49128ee676fb86e0b874",
            "64f64ed846ea47aeaa8d856a03eddfc4",
            "b30c9b88c1884661b7b8d8471efee5ed",
            "e61c11ab9dff421ba36169faad5f14b4",
            "acc509904cbf4e098d5db5256d0ffc6d",
            "1b68285265bf4ffea3e257ed59ffeb3f",
            "29ea44a691e845b78e40d0c53fb42d1f",
            "a485ca524ad74ad294f3ca2a7deca63c",
            "590abea13c484a65a68331fde8ef7a83",
            "5f55922252ea475b80c40c14319c86b5",
            "a819e10fe5004940aadf364b9e8aced4",
            "d7cab69bdb5d4590abda56cab691c0b4",
            "3765a0df35144b74aad4c2d0ec9fd7a2",
            "98eea8b2bb614e98898170cbbe9ebed9",
            "53e685182d464b9b9c4eed74d537ddd0",
            "65ec33da2b5f4952895ac4c6cce06c2d",
            "61ef2a982e0c4a6e859eb40195461653",
            "0e1d21da0fcb4619a7201f79054e9ef2",
            "a6f2024cae404f4d8938f2fce1aba945",
            "8ece9e92a15549c0bc2a92a17d8238d2",
            "a306353b23bf49958b62ae68d137075e",
            "e77f5a2641da4edd9f56e78a99763d48",
            "969076ead6c04bdc910e17c632d36000",
            "d4a3471e733b4099ad64be27b4690850",
            "64d1d051a42742818530aaa329a1b57c",
            "7ce573546edc4953b1f958efb4907068",
            "958f45d2fab74b9b9343ee2c995c91ca",
            "433f2a42134f43668313a94344bfc3ae",
            "36201a96150e46ee8758d161ec79a359",
            "cca8b5990a7944d9b451b09bd9cb37d7",
            "a71c9cd217cf4795b8377463548721fe",
            "86137c75a8484e0f9e33e39efdd40ba1",
            "20987be84227409b890548d4ea15441b"
          ]
        },
        "outputId": "efce5706-878e-450c-e0cb-687deff36a57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60b026e48c7c4d50982b0e8b05561511"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "683f7b2f3bba49128ee676fb86e0b874"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7cab69bdb5d4590abda56cab691c0b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "969076ead6c04bdc910e17c632d36000"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the ü§ó Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2zMp5qC9Li-"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtdVPfbq9Li-"
      },
      "source": [
        "You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5hBlsrHIrJL",
        "outputId": "0a9313c7-3df2-4072-8d0d-4ba5122344b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "**input_ids**': This is a list of integer indices representing the tokens. Each integer corresponds to a specific token in the vocabulary.\n",
        "\n",
        "'**attention_mask**': This is a binary mask indicating which elements in the input sequence are padding (0) and which are actual tokens (1). It helps the model distinguish between the actual input tokens and padding tokens.\n",
        "\n",
        "**Input Tokens:** These are the actual tokens representing the meaningful content of the text.\n",
        "\n",
        "**Padding Tokens:** These are additional tokens (usually zeros) added to make all input sequences in a batch have the same length.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oGAhymt9Li_"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These parameters are commonly used in the context of handling long documents or passages during tokenization and input preparation for transformer-based models.\n",
        "\n",
        "**`max_length:`**\n",
        "\n",
        "**Definition**: It represents the maximum length (in terms of tokens) that a feature, which includes both the question and context, should have.\n",
        "\n",
        "**Purpose**: In question answering tasks, the input text is often composed of a question and a context. The max_length parameter sets a limit on the total number of tokens allowed in the combined question and context\n",
        "\n",
        "doc_stride:\n",
        "\n",
        "\n",
        "Purpose: When tokenizing long documents or passages, it's common to split them into chunks to fit within the model's maximum token limit. doc_stride determines the overlap between consecutive chunks.\n",
        "\n",
        "**EX**:\n",
        "The first segment includes tokens 1 to 384.\n",
        "The second segment includes tokens 256 to 640 (overlapping with the first segment by 128 tokens)."
      ],
      "metadata": {
        "id": "vWNPWFc6xJdE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OFQJQVG9LjC"
      },
      "source": [
        "Let's find one long example in our dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the length is greater than a certain threshold (384 in this case), it may trigger actions like truncation or segmentation to fit the model's constraints."
      ],
      "metadata": {
        "id": "67pW4NLtl8-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gr1Mef29LjC"
      },
      "outputs": [],
      "source": [
        "for i, example in enumerate(datasets[\"train\"]):\n",
        "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
        "        break\n",
        "example = datasets[\"train\"][i]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code  is **iterating through the training examples in the dataset (datasets[\"train\"]) and finding the first example where the tokenized representation of the combined question and context exceeds a maximum length of 384 tokens**."
      ],
      "metadata": {
        "id": "f2TVqpE0kqlh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjblcIti9LjC"
      },
      "source": [
        "Without any truncation, we get the following length for the input IDs:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculates the length (number of tokens) of the tokenized input sequence for a specific example"
      ],
      "metadata": {
        "id": "Z0nCDeNWymTb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIlUgdtK9LjC",
        "outputId": "74dd581a-4b3c-488e-dff1-611cde9477b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "396"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oug3iw-q9LjC"
      },
      "source": [
        "Now, if we just truncate, we will lose information (and possibly the answer to our question):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculates the length of the tokenized representation of the question and context while taking into account the specified maximum length and truncation strategy.\n",
        "\n",
        "This is commonly used when you want to ensure that the combined length of the question and context does not exceed a certain limit (max_length)"
      ],
      "metadata": {
        "id": "iGXOimZd367Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WTez4hu9LjC",
        "outputId": "d046dde4-98b2-461b-aa22-6eccd4916481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i1fGkb49LjD"
      },
      "source": [
        "Note that we never want to truncate the question, only the context, else the `only_second` truncation picked. Now, our tokenizer can automatically return us a list of features capped by a certain maximum length, with the overlap we talked above, we just have to tell it with `return_overflowing_tokens=True` and by passing the stride:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPkTOysQ9LjD"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`truncation`**\n",
        "\n",
        "refers to the process of shortening a sequence of text by removing a portion of it. This is often necessary when the original text is too long to fit within the maximum length constraints\n",
        "\n",
        "ex\n",
        "\n",
        "```\n",
        "# Question: \"What is the capital of France?\"\n",
        "\n",
        "Context: \"Paris, the beautiful capital of France, is known for its iconic landmarks...\"\n",
        "\n",
        "[CLS], \"What\", \"is\", \"the\", \"capital\", \"of\", \"France\", \"?\", [SEP], \"Paris\", \",\", \"the\", \"beautiful\", \"capital\", \"of\", \"France\", \",\", \"is\", \"known\", \"for\", \"its\", \"iconic\", \"landmarks\", \"...\", [SEP]\n",
        "\n",
        " truncation=\"only_second\"\n",
        "\n",
        "\n",
        "[CLS], \"What\", \"is\", \"the\", \"capital\", \"of\", \"France\", \"?\", [SEP], \"Paris\", \",\", \"the\", \"beautiful\", \"...\", [SEP]\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I2ejYhIZnv3e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jc-Jy-N9LjD"
      },
      "source": [
        "Now we don't have one list of `input_ids`, but several:\n",
        "\n",
        "\n",
        "creates a list containing the length of each sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Uc9Ov59LjD",
        "outputId": "aa11e0ce-0d8d-4fc5-d30f-4daa40fa80ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[384, 157]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "[len(x) for x in tokenized_example[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KofuP_gV9LjD"
      },
      "source": [
        "And if we decode them, we can see the overlap:\n",
        "\n",
        "iterates over the first two tokenized input sequences in tokenized_example[\"input_ids\"] and prints the human-readable text by decoding each sequence using the tokenizer's decode method.\n",
        "\n",
        "Output:\n",
        "\n",
        "The human-readable text representation of each tokenized sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zesToo-P9LjD",
        "outputId": "39de7e44-7ca4-4c7e-f90e-bdc0824fde49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 ‚Äì 2010 season. the team is coached by mike brey, who, as of the 2014 ‚Äì 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 ‚Äì 11 team concluded its regular season ranked number seven in the country, with a record of 25 ‚Äì 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
            "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 ‚Äì 11 team concluded its regular season ranked number seven in the country, with a record of 25 ‚Äì 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
          ]
        }
      ],
      "source": [
        "for x in tokenized_example[\"input_ids\"][:2]:\n",
        "    print(tokenizer.decode(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8boB4Ehz9LjD"
      },
      "source": [
        "Now this will give us some work to properly treat the answers: we need to find in which of those features the answer actually is, and where exactly in that feature. The models we will use require the start and end positions of these answers in the tokens, so we will also need to to map parts of the original context to some tokens. Thankfully, the tokenizer we're using can help us with that by returning an `offset_mapping`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmx93T2i9LjE",
        "outputId": "be98e324-f3d1-4220-f4fb-9e814338e70a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n"
          ]
        }
      ],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    stride=doc_stride\n",
        ")\n",
        "print(tokenized_example[\"offset_mapping\"][0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XBBChol9LjE"
      },
      "source": [
        "**`offset mapping`**\n",
        "\n",
        "**`provides the start and end character positions of the token in the original text.`**\n",
        "\n",
        " provides a mapping between the tokens in the tokenized sequence and their corresponding character positions in the original text.\n",
        "\n",
        "**stride=doc_stride**: This parameter determines the overlap between consecutive chunks when splitting a long document\n",
        "\n",
        "\n",
        "tokenizes the question and context of a specific example using the tokenizer with additional parameters for handling long documents or passages.\n",
        "\n",
        "**max_length=max_length**\n",
        "\n",
        "If the total number of tokens after tokenization exceeds this maximum length, the sequence is truncated or split accordingly\n",
        "\n",
        " \"**truncation**\" refers to the process of shortening a sequence of tokens to fit within a specified maximum length.\n",
        "\n",
        " \"**only_second**\" means that the truncation will be applied to the second input sequence (in this case, the \"context\").\n",
        "\n",
        "`**return_overflowing_tokens=True**`\n",
        "\n",
        "return any overflowing tokens that do not fit into the specified maximum length return_overflowing_tokens=True, the tokenizer will provide these additional parts, allowing you to handle cases where the input exceeds the model's maximum token limit.\n",
        "The additional parts are accessible in the overflowing_tokens field in the output.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okBL8lku9LjE",
        "outputId": "88c48ab1-a390-4a9c-9cca-8f3cde1efa52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how How\n"
          ]
        }
      ],
      "source": [
        "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
        "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
        "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "first_token_id = tokenized_example[\"input_ids\"][0][1]:\n",
        "\n",
        "Retrieves the ID of the second token (index 1) in the first tokenized sequence. The index 0 often corresponds to the [CLS] token.\n",
        "offsets = tokenized_example\n",
        "\n",
        "[\"offset_mapping\"][0][1]:\n",
        "\n",
        "Retrieves the offset mapping for the second token in the first tokenized sequence. The offset mapping provides the start and end character positions of the token in the original text.\n",
        "\n",
        "tokenizer.convert_ids_to_tokens([first_token_id])[0] to convert the token ID back to its textual representation."
      ],
      "metadata": {
        "id": "iyL4EqTHtAqg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2WywB3X9LjE"
      },
      "source": [
        "So we can use this mapping to find the position of the start and end tokens of our answer in a given feature. We just have to distinguish which parts of the offsets correspond to the question and which part correspond to the context, this is where the `sequence_ids` method of our `tokenized_example` can be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy_zG21P9LjE",
        "outputId": "55161096-411c-4c47-9f6f-e6b668a3bba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
          ]
        }
      ],
      "source": [
        "sequence_ids = tokenized_example.sequence_ids()\n",
        "print(sequence_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMgDuxp09LjE"
      },
      "source": [
        "**sequence_ids** method returns a list of sequence IDs for each token in the tokenized example\n",
        "\n",
        "It returns `None` for the special tokens,\n",
        "\n",
        "then **`0`** corresponding token comes from the first sentence past (the question)\n",
        "\n",
        " or **`1`** depending on whether the corresponding token comes from the first  the second (the context).\n",
        "\n",
        "  Now with all of this, we can find the first and last token of the answer in one of our input feature (or if the answer is not in this feature):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsMR7O689LjE",
        "outputId": "65f35d70-9f7c-4a04-cc34-a265880e7aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23 26\n"
          ]
        }
      ],
      "source": [
        "#Retrieve Answer Information:\n",
        "#nswers contains information about the answer, including the starting character position (start_char) and the ending character position (end_char).\n",
        "answers = example[\"answers\"]\n",
        "start_char = answers[\"answer_start\"][0]\n",
        "end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "# These while loops find the token indices corresponding to the start and end character positions of the answer in the tokenized sequence.\n",
        "token_start_index = 0\n",
        "while sequence_ids[token_start_index] != 1:\n",
        "    token_start_index += 1\n",
        "\n",
        "# End token index of the current span in the text.\n",
        "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
        "while sequence_ids[token_end_index] != 1:\n",
        "    token_end_index -= 1\n",
        "\n",
        "\n",
        "\n",
        "#send to omar\n",
        "# if the answer span is within the span of the tokenized sequence by comparing character positions with offset mappings\n",
        "offsets = tokenized_example[\"offset_mapping\"][0]\n",
        "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
        "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "        token_start_index += 1\n",
        "    start_position = token_start_index - 1\n",
        "    while offsets[token_end_index][1] >= end_char:\n",
        "        token_end_index -= 1\n",
        "    end_position = token_end_index + 1\n",
        "    print(start_position, end_position)\n",
        "else:\n",
        "    print(\"The answer is not in this feature.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuaowvsxYX88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "align the tokenized answer with the original text** by finding the corresponding token indices. It considers cases where the answer might extend beyond the boundaries of the feature"
      ],
      "metadata": {
        "id": "N6bb1CbSDFqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAzfTuDY9LjF"
      },
      "source": [
        "print the decoded representation of the answer span in the tokenized sequence and compare it with the original answer tex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xovCMGmg9LjF",
        "outputId": "4fd9f403-94e2-400d-de32-a8e0d3ff3a65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "over 1, 600\n",
            "over 1,600\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
        "print(answers[\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O529aVja9LjF"
      },
      "source": [
        "For this notebook to work with any kind of models, we need to account for the special case where the model expects padding on the left (in which case we switch the order of the question and the context):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In natural language processing tasks, when preparing input sequences for a model, padding is often applied to ensure that all sequences in a batch have the same length. The side on which the padding is applied can affect how the model processes the input. This variable, pad_on_right, can be used later in the code to handle cases where padding is applied on the right side of the input sequences."
      ],
      "metadata": {
        "id": "TZXsUQSzxM8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXKJb3k49LjF"
      },
      "outputs": [],
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0UQJHb39LjF"
      },
      "source": [
        "Now let's put everything together in one function we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the cls index for both the start and end position. We could also simply discard those examples from the training set if the flag `allow_impossible_answers` is `False`. Since the preprocessing is already complex enough as it is, we've kept is simple for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNJbMASp9LjF"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  tokenized_subset = squad_subset.map(prepare_train_features, batched=True, remove_columns=squad_subset.column_names)\n"
      ],
      "metadata": {
        "id": "fZiHh16DdBr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " preparing data for training a question-answering model. It handles the tokenization, truncation, padding, and labeling of examples, taking into account the specifics of the tokenization process and the requirements of the downstream task"
      ],
      "metadata": {
        "id": "NIlpflf5LQcs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b70jh26IrJS"
      },
      "outputs": [],
      "source": [
        "features = prepare_train_features(datasets['train'][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare_train_features function is applied to the first 5 examples from the training dataset\n",
        "\n",
        "it processes and tokenizes a subset of training examples to prepare them for training a question-answering mode"
      ],
      "metadata": {
        "id": "yNthbeIQLmrT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command. Since our preprocessing changes the number of samples, we need to remove the old columns when applying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "# Assuming you want to transform the \"train\" subset\n",
        "# tokenized_datasets = datasets[\"train\"].map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "TDTqP656nrja",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387,
          "referenced_widgets": [
            "b4bd895d71744bbc82b40803554269bf",
            "d01f5e67e9334334be0ff5f188829c2e",
            "4aac69f8d94146f994286bca547d788f",
            "87d5095234304e0ea4f3f99a310bcd55",
            "8851916313f4475e94b972fe096c800e",
            "bb50d0f182fc48f28f8211677715eed2",
            "f22995e46aba47b1816dc98d6a932eb0",
            "761f312ad1784e58b0ee2dfd0bc47f7b",
            "f26eca8df825485b94c40cbebdc43da9",
            "0913bb55fe364b9b958727f98d41ac4e",
            "6e6fc322874d444b83803b0f1861f1f3"
          ]
        },
        "outputId": "b270d7d2-033c-46c0-f8d1-30c1a48bfd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4bd895d71744bbc82b40803554269bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ef179c923c46>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 868\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    869\u001b[0m                 k: dataset.map(\n\u001b[1;32m    870\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m         return DatasetDict(\n\u001b[1;32m    868\u001b[0m             {\n\u001b[0;32m--> 869\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    870\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3091\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3092\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3093\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3094\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3468\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3469\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3470\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3471\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m                 processed_inputs = {\n",
            "\u001b[0;32m<ipython-input-35-1ef03274a3a5>\u001b[0m in \u001b[0;36mprepare_train_features\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# in one example possible giving several features when a context is long, each of those features having a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# context that overlaps a bit the context of the previous feature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     tokenized_examples = tokenizer(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpad_on_right\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpad_on_right\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2798\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2882\u001b[0m                 )\n\u001b[1;32m   2883\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2884\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2885\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3073\u001b[0m         )\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3076\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    502\u001b[0m         )\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Even better, the results are automatically cached by the ü§ó Datasets library to avoid spending time on this step the next time you run your notebook. The ü§ó Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ü§ó Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming datasets is already loaded\n",
        "print(len(datasets[\"train\"]), len(datasets[\"validation\"]))\n"
      ],
      "metadata": {
        "id": "6ILC51G4nELv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "percentage split is **`89.23% for training`** and **`10.77% for validation`** in the SQuAD dataset.\n",
        "\n",
        "\n",
        "https://www.tensorflow.org/datasets/catalog/squad#:~:text=Stanford%20Question%20Answering%20Dataset%20(SQuAD,the%20question%20might%20be%20unanswerable.&text=Versions%3A,3.0."
      ],
      "metadata": {
        "id": "eGKglkBDtT3P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`AutoModelForQuestionAnswering`**\n",
        "\n",
        "automatically load a pre-trained question-answering (QA) model based on a provided model identifier or path.\n",
        "\n",
        "**`from_pretrained`**\n",
        "\n",
        " method allows users to load models using either a model identifier or a local path. Model identifiers can be names like \"distilbert-base-uncased-distilled-squad\" or paths to locally stored models."
      ],
      "metadata": {
        "id": "yPT1dyMnGEVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "xEUcLM3cIAxm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`accelerate`** is a library developed by Hugging Face to simplify and optimize the training of deep learning models\n",
        "\n",
        "you can train your models on multiple GPUs or even multiple machines."
      ],
      "metadata": {
        "id": "b0AO_trAH8WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "7oXz2qMWBmro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "To9g15q2Jd92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install transformers[torch] ensures that you have both the transformers library and the PyTorch dependencies installed\n",
        "\n",
        "[**`torch`**]: This is an extra specifier that indicates you want to install additional dependencies for PyTorch."
      ],
      "metadata": {
        "id": "uoQRMJHKJc2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "Wa2Xx2lKB6KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "VtJ89mAgC62L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[pytorch]"
      ],
      "metadata": {
        "id": "YuOijaNxDpXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "\n",
        "accelerate.__version__"
      ],
      "metadata": {
        "id": "Lz3W1GibD-N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "import transformers\n",
        "\n",
        "transformers.__version__, accelerate.__version__"
      ],
      "metadata": {
        "id": "wmvVcQ3jEG-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`TrainingArguments`** class. This class is part of the transformers library and is used to **store all the hyperparameters** and **settings for training a model.**\n",
        "\n",
        "**`evaluation_strategy=\"epoch\":`**  evaluation should be** performed at the end of each epoch** during training.\n",
        "\n",
        "**`learning rate`** is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
        "\n",
        "**`per_device_train_batch_size`** and **`per_device_eval_batch_size`** in the training configuration (TrainingArguments) are used to specify the batch size per device during training and evaluation, respectively"
      ],
      "metadata": {
        "id": "gd7b_PqPMM23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n"
      ],
      "metadata": {
        "id": "NEAZ8h6inOwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "46kPu1rjnPZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p.predictions, p.label_ids\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Flatten predictions and labels\n",
        "    prediction_list = [pred for pred_seq in predictions for pred in pred_seq]\n",
        "    label_list = [label for label_seq in labels for label in label_seq]\n",
        "\n",
        "    # Make sure the lengths match\n",
        "    assert len(prediction_list) == len(label_list), \"Number of predictions and labels must match\"\n",
        "\n",
        "    # Compute F1 and EM scores\n",
        "    f1 = f1_score(label_list, prediction_list, average='micro')\n",
        "    em = accuracy_score(label_list, prediction_list)\n",
        "\n",
        "    return {\"f1\": f1, \"exact_match\": em}\n"
      ],
      "metadata": {
        "id": "5kWm6GeanRWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# > early_stopping\n",
        "\n"
      ],
      "metadata": {
        "id": "XN3XpeWJjeZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.trainer_callback import EarlyStoppingCallback\n"
      ],
      "metadata": {
        "id": "mFrYWjRtnTB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=1,\n",
        "    early_stopping_threshold=0.0125,\n",
        ")\n"
      ],
      "metadata": {
        "id": "IUtgHnyVnUls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    #This is a formatted string that defines the directory where the fine-tuned model and related outputs will be saved.\n",
        "    f\"{model_name}-finetuned-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    #the speed at which a machine learning model \"learns\"\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    #a technique used to prevent overfitting,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay.\n",
        "\n",
        "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-squad\"` or `\"huggingface/bert-finetuned-squad\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jdPejSf9LjH"
      },
      "source": [
        "Then we will need a data collator that will batch our processed examples together, here the default one will work:\n",
        "\n",
        "**` data collator`** is a function or object responsible for combining individual training examples into batches. The purpose of a data collator is to take a list of samples and organize them into a batch that can be efficiently processed by the model during training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# train\n",
        "\n"
      ],
      "metadata": {
        "id": "tKzIg5qejlJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKzcXkH79LjH"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "We will evaluate our model and compute metrics in the next section (this is a very long operation, so we will only compute the evaluation loss during training).\n",
        "\n",
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Trainer`** class in the transformers library provides a high-level interface for training and evaluating transformer models.\n",
        "\n",
        "Trainer simplifies the process of fine-tuning transformer models, making it easier for users to experiment with different architectures and hyperparameters\n",
        "\n",
        "**`Training Arguments (args`**): An object containing various hyperparameters and settings for training. This includes parameters such as learning rate, batch size, number of epochs, and more.\n",
        "\n",
        "**`Data Collator (data_collator)`**: A data collator is responsible for batching and organizing individual examples into input batches that can be processed by the model\n",
        "\n",
        "**`Tokenizer (tokenizer)`**: The tokenizer used to convert raw text into tokenized input suitable for the model. It is responsible for encoding text into input IDs and attention masks."
      ],
      "metadata": {
        "id": "ovc_GEE9TKEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMbsX5ho9LjI"
      },
      "source": [
        "Since this training is particularly long, let's save the model just in case we need to restart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After trainer.train()\n",
        "best_checkpoint = trainer.state.best_model_checkpoint\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(best_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(best_checkpoint)\n"
      ],
      "metadata": {
        "id": "n68K_5qkoB6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = [1, 2, 3]\n",
        "training_loss = [1.223600, 0.957300, 0.751500]\n",
        "validation_loss = [1.178746, 1.130409, 1.166829]\n",
        "\n",
        "# Plotting\n",
        "plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
        "plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEir4KfjXXT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6vADW1O9LjI"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QqGzIb89LjI"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9oUHM5q9LjI"
      },
      "source": [
        "Evaluating our model will require a bit more work, as we will need to map the predictions of our model back to parts of the context. The model itself predicts logits for the start and en position of our answers: if we take a batch from our validation datalaoder, here is the output our model gives us:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`perform evaluation on a batch of data.`**\n",
        "\n",
        "means using the trained model **to make predictions on a set of input examples** for the purpose of assessing the model's performance."
      ],
      "metadata": {
        "id": "x-J2OE_8CF0o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5nAQ10K9LjI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    output = trainer.model(**batch)\n",
        "output.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`logits`**\n",
        "\n",
        "They are the outputs of a neural network before applying an activation function, such as softmax. Logits can be seen as the scores assigned to each class in a classification problem or the unnormalized scores for different positions in a sequence."
      ],
      "metadata": {
        "id": "UsOxCIaXENb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking the shape of the logits produced by the model for start and end positions\n",
        " of answer in the context"
      ],
      "metadata": {
        "id": "8AaoqtfmDpBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Start Logits:`**\n",
        "\n",
        "indicate how likely it is to be the beginning of the answer.\n",
        "\n",
        "**`End Logits:`**\n",
        "\n",
        " indicate how likely it is to be the end of the answer.\n",
        "\n"
      ],
      "metadata": {
        "id": "XPIJn_6FHNCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2rOGDHN9LjI"
      },
      "outputs": [],
      "source": [
        "output.start_logits.shape, output.end_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " used to **`find the positions (indices) in the sequence with the highest logit`** values. Specifically,\n",
        "\n",
        "  argmax(dim=-1) is applied along the last dimension of the logits tensor, which corresponds to the different positions in the sequence."
      ],
      "metadata": {
        "id": "9AZLRtDOE4rq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKcoNuuA9LjJ"
      },
      "outputs": [],
      "source": [
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WENeNC519LjJ"
      },
      "source": [
        "This will work great in a lot of cases, but what if this prediction gives us something impossible: the start position could be greater than the end position, or point to a span of text in the question instead of the answer. In that case, we might want to look at the second best prediction to see if it gives a possible answer and select that instead.\n",
        "\n",
        "However, picking the second best answer is not as easy as picking the best one: is it the second best index in the start logits with the best index in the end logits? Or the best index in the start logits with the second best index in the end logits? And if that second best answer is not possible either, it gets even trickier for the third best answer.\n",
        "\n",
        "\n",
        "To classify our answers, we will use the score obtained by adding the start and end logits. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call `n_best_size`. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one. Here is how we would do this on the first feature in the batch:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if n_best_size is set to 20, the post-processing step will **`consider the top 20 candidate answer`** spans based on their scores\n",
        "\n",
        "\n",
        "**`Candidate Answer Spans`**\n",
        "\n",
        "These are possible answers that the model identifies based on the logits (scores) assigned to different positions in the input context."
      ],
      "metadata": {
        "id": "dfyYlXTQF_9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X0Hyqur9LjJ"
      },
      "outputs": [],
      "source": [
        "n_best_size = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`1-`**Convert start_logits and end_logits to NumPy arrays for easier manipulation.\n",
        "\n",
        "**`2-`**Get indices of the top candidates:\n",
        "\n",
        "**`3-`**Generate valid answers: It iterates through combinations of start and end indices, checking if the start index is less than or equal to the end index. This ensures a valid answer span\n",
        "\n",
        "check **`ensures that the start index of a candidate answer comes before or at the same position as the end index`**\n",
        "\n"
      ],
      "metadata": {
        "id": "e0-hhfvHIpG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLnmVJDh9LjJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#Convert start_logits and end_logits to NumPy arrays for easier manipulation.\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "#Get indices of the top candidates:\n",
        "#it gets the indices of the top candidates in descending order.\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "\n",
        "#Generate valid answers: It iterates through combinations of start and end indices,\n",
        "#checking if the start index is less than or equal to the end index. This ensures a valid answer span\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
        "                }\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOCDW7Vg9LjJ"
      },
      "source": [
        "And then we can sort the `valid_answers` according to their `score` and only keep the best one. The only point left is how to check a given span is inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
        "- the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "- the offset mapping that will give us a map from token indices to character positions in the context.\n",
        "\n",
        "That's why we will re-process the validation set with the following function, slightly different from `prepare_train_features`:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`this function prepares validation features by tokenizing the questions and contexts, handling overflow, and maintaining mappings between features and examples. It ensures that the tokenized validation data is appropriately formatted for evaluation during the fine-tuning process.`**"
      ],
      "metadata": {
        "id": "61rVPVpNM5qc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UJ6jRaa9LjJ"
      },
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "\n",
        "    #Whitespace Removal:ensures consistent formatting and helps with tokenization.\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAghGd7K9LjK"
      },
      "source": [
        "And like before, we can apply that function to our validation set easily:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`prepare_validation_features:`**\n",
        "\n",
        "The function used to process each example in the validation dataset.\n",
        "\n",
        "**`batched=True: `**\n",
        "\n",
        "Indicates that the function should be applied in a batched manner, which can **improve efficiency during processing**\n",
        "\n",
        "**`remove_columns=datasets[\"validation\"].column_names: `**\n",
        "\n",
        "**Removes the specified columns** from the processed features. This is done to **clean up unnecessary information and reduce memory usage.**\n",
        "\n",
        "**`validation_features`**:\n",
        "\n",
        "Stores the processed validation features."
      ],
      "metadata": {
        "id": "aEnzRgMMOWmt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idPeNLgI9LjK"
      },
      "outputs": [],
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`validation_features,`** **is a tokenized and formatted version of the validation dataset, ready to be used for model evaluation**. It contains information such as input IDs, attention masks, offset mappings, and example IDs. During evaluation, this preprocessed dataset is fed into the fine-tuned model to obtain predictions, which can then be compared with the ground truth to assess the model's performance on the validation set."
      ],
      "metadata": {
        "id": "EY_CgVhdP3fH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx15RTeD9LjK"
      },
      "source": [
        "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jREGk5y9LjK"
      },
      "outputs": [],
      "source": [
        "raw_predictions = trainer.predict(validation_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3MuHNDg9LjK"
      },
      "source": [
        "The `Trainer` *hides* the columns that are not used by the model (here `example_id` and `offset_mapping` which we will need for our post-processing), so we set them back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XITiubDy9LjK"
      },
      "outputs": [],
      "source": [
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFvx_DD19LjK"
      },
      "source": [
        "We can now refine the test we had before: since we set `None` in the offset mappings when it corresponds to a part of the question, it's easy to check if an answer is fully inside the context. We also eliminate very long answers from our considerations (with an hyper-parameter we can tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXTqwuRR9LjL"
      },
      "outputs": [],
      "source": [
        "max_answer_length = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Offset Mappings`**\n",
        "\n",
        "ex\n",
        "running\" in the tokenized input corresponds to the characters 10 to 16 in the original text, the offset mapping for that token would be [(10, 16)]."
      ],
      "metadata": {
        "id": "Av_YIhf8Thdg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrtzzCsz9LjL"
      },
      "outputs": [],
      "source": [
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "\n",
        "#It fetches the original context (the passage of text) from the validation dataset.\n",
        "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
        "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
        "# an example index\n",
        "context = datasets[\"validation\"][0][\"context\"]\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "#Filtering and Forming Valid Answers:\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "        # to part of the input_ids that are not in the context.\n",
        "        if (\n",
        "            start_index >= len(offset_mapping)\n",
        "            or end_index >= len(offset_mapping)\n",
        "            or offset_mapping[start_index] is None\n",
        "            or offset_mapping[end_index] is None\n",
        "        ):\n",
        "            continue\n",
        "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "            continue\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            start_char = offset_mapping[start_index][0]\n",
        "            end_char = offset_mapping[end_index][1]\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": context[start_char: end_char]\n",
        "                }\n",
        "            )\n",
        "#Selecting Top Answers:\n",
        "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "valid_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIFQdxEw9LjL"
      },
      "source": [
        "We can compare to the actual ground-truth answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it includes information about the text of the answer and the character position where the answer starts in the original context."
      ],
      "metadata": {
        "id": "K9uQq9b8VUXh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln3khiY39LjM"
      },
      "outputs": [],
      "source": [
        "datasets[\"validation\"][0][\"answers\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX2iQ9Av9LjM"
      },
      "source": [
        "Our model picked the right as the most likely answer!\n",
        "\n",
        "As we mentioned in the code above, this was easy on the first feature because we knew it comes from the first example. For the other features, we will need a map between examples and their corresponding features. Also, since one example can give several features, we will need to gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`example_id_to_index:`**\n",
        "\n",
        "It creates a dictionary that maps example IDs to their corresponding index in the validation dataset.\n",
        "\n",
        "**`features_per_example:`**\n",
        "\n",
        "It is a defaultdict that collects lists of feature indices for each example."
      ],
      "metadata": {
        "id": "7We9PQbyWA_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of organizing the features in this way is to **facilitate the grouping of features based on the example** to which they belong"
      ],
      "metadata": {
        "id": "vpJNWnv0WO9S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyphHcEV9LjM"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "examples = datasets[\"validation\"]\n",
        "features = validation_features\n",
        "\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "features_per_example = collections.defaultdict(list)\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqNQTIa9LjM"
      },
      "source": [
        "We're almost ready for our post-processing function. The last bit to deal with is the impossible answer (when `squad_v2 = True`). The code above only keeps answers that are inside the context, we need to also grab the score for the impossible answer (which has start and end indices corresponding to the index of the CLS token). When one example gives several features, we have to predict the impossible answer when all the features give a high score to the impossible answer (since one feature could predict the impossible answer just because the answer isn't in the part of the context it has access too), which is why the score of the impossible answer for one example is the *minimum* of the scores for the impossible answer in each feature generated by the example.\n",
        "\n",
        "We then predict the impossible answer when that score is greater than the score of the best non-impossible answer. All combined together, this gives us this post-processing function:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        if not squad_v2:\n",
        "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "        else:\n",
        "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "            predictions[example[\"id\"]] = answer\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "kpezpfkjvltX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xjXXOYa9LjM"
      },
      "source": [
        "And we can apply our post-processing function to our raw predictions:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "ugPhUf5ni5hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply validation"
      ],
      "metadata": {
        "id": "FIqnR-Pvgf31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
      ],
      "metadata": {
        "id": "JWAJqJlSvqDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWV56RmV9LjN"
      },
      "source": [
        "Then we can load the metric from the datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVNcda3C9LjN"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoOgdklE9LjN"
      },
      "source": [
        "Then we can call compute on it. We just need to format predictions and labels a bit as it expects a list of dictionaries and not one big dictionary. In the case of squad_v2, we also have to set a `no_answer_probability` argument (which we set to 0.0 here as we have already set the answer to empty if we picked it)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if squad_v2:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
        "else:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ],
      "metadata": {
        "id": "XaDe8lfJv05d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`exact match`** score of 73.12% indicates **`the percentage of predictions that exactly match the ground truth answers`**"
      ],
      "metadata": {
        "id": "O4hSSH_Kfe2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = {'exact_match': 76.81173131504258, 'f1': 85.18108051522785}\n",
        "\n",
        "# Plotting the metrics\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['blue', 'green'])\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Exact Match and F1 Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HeeMAVIUW0bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-wo4C67emxRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "CvuPNMtmTIFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# TESTING\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F2qXoTRVYNuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your training code here\n",
        "\n",
        "# Save the fine-tuned model\n",
        "save_path = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n"
      ],
      "metadata": {
        "id": "M8D_nD4TAIjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load the fine-tuned DistilBERT model and tokenizer\n",
        "fine_tuned_model = AutoModelForQuestionAnswering.from_pretrained(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\")\n"
      ],
      "metadata": {
        "id": "9R0DN0iQASkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade\n"
      ],
      "metadata": {
        "id": "XPQSNY2PBqt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Load the fine-tuned DistilBERT model and tokenizer\n",
        "model_name = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "def generate_answer_distilbert_squad(row):\n",
        "    question = row[\"question\"]\n",
        "    context = row[\"context\"]\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=384,  # Adjust the max length as per your fine-tuning\n",
        "        padding=\"max_length\",\n",
        "        truncation=\"only_second\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bs, q_length, dim = encoding[\"input_ids\"].size() if len(encoding[\"input_ids\"].size()) == 3 else encoding[\"input_ids\"].size(0), encoding[\"input_ids\"].size(-2), encoding[\"input_ids\"].size(-1)\n",
        "\n",
        "        start_logits = output.start_logits.argmax(dim=-1)\n",
        "        end_logits = output.end_logits.argmax(dim=-1)\n",
        "\n",
        "        start_index = torch.argmax(start_logits).item()\n",
        "        end_index = torch.argmax(end_logits).item()\n",
        "\n",
        "    answer = tokenizer.decode(encoding[\"input_ids\"][0][start_index:end_index+1], skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        \"context\": context,\n",
        "        \"question\": question,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "# Example usage with a row from your SQuAD dataset\n",
        "sample_row = {\n",
        "    \"question\": \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
        "    \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend 'Venite Ad Me Omnes'. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\",\n",
        "    \"id\": \"5733be284776f41900661182\",\n",
        "}\n",
        "\n",
        "result = generate_answer_distilbert_squad(sample_row)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "Y33H8tCj_jdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n"
      ],
      "metadata": {
        "id": "Ws9tGWwC0cii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "_axbKKxt0eTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_example = \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\"...\"\n",
        "\n",
        "question_example = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "\n",
        "answer_example = get_answer(context_example, question_example)\n",
        "\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Answer: {answer_example}\")\n"
      ],
      "metadata": {
        "id": "SqyZuCKV0eQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "QdvJjXib2foD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Create a function for question answering\n",
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n",
        "\n",
        "# Test the function with a context and question\n",
        "context_example = \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\n",
        "question_example = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "answer_example = get_answer(context_example, question_example)\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Answer: {answer_example}\")\n"
      ],
      "metadata": {
        "id": "FG_MxCNlYPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Answer: {answer_example}\")"
      ],
      "metadata": {
        "id": "fOyIYizXT14x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Create a function for question answering\n",
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n",
        "\n",
        "# New context and question\n",
        "new_context = \"Paris, the capital of France, is renowned for its architecture, art, and rich history. The Eiffel Tower, an iconic landmark, stands tall on the Champ de Mars, offering breathtaking views of the city. The Louvre Museum, home to thousands of works of art, including the Mona Lisa, attracts millions of visitors each year. Paris is known for its charming streets, sidewalk cafes, and vibrant cultural scene.\"\n",
        "\n",
        "new_question = \"What is the most famous landmark in Paris?\"\n",
        "new_answer = get_answer(new_context, new_question)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Question: {new_question}\")\n",
        "print(f\"Answer: {new_answer}\")\n"
      ],
      "metadata": {
        "id": "oKvQ4d2PHW1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Create a function for question answering\n",
        "def get_answer(context, question):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "    return answer\n",
        "\n",
        "# New context and question\n",
        "#new_context = \"Paris, the capital of France, is renowned for its architecture, art, and rich history. The Eiffel Tower, an iconic landmark, stands tall on the Champ de Mars, offering breathtaking views of the city. The Louvre Museum, home to thousands of works of art, including the Mona Lisa, attracts millions of visitors each year. Paris is known for its charming streets, sidewalk cafes, and vibrant cultural scene.\"\n",
        "\n",
        "new_question = \"During whose rule was the use of Old Akkadian at its peak?\"\n",
        "new_answer = get_answer(new_context, new_question)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Question: {new_question}\")\n",
        "print(f\"Answer: {new_answer}\")\n"
      ],
      "metadata": {
        "id": "MJg2UiVeg6uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "\n",
        "# Iterate through SQuAD examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "1IXOBcv5koUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load SQuAD 2.0 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Print the first example in the training set\n",
        "print(squad_dataset[\"train\"][0])\n"
      ],
      "metadata": {
        "id": "gq2FjYeIrD0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(squad_dataset[\"train\"][i])\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Add a separator for better readability"
      ],
      "metadata": {
        "id": "si-PYJkJrZpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Print 10 random examples from SQuAD v2 dataset\n",
        "for _ in range(10):\n",
        "    random_index = random.randint(0, len(squad_dataset[\"train\"]) - 1)\n",
        "    example = squad_dataset[\"train\"][random_index]\n",
        "\n",
        "    print(example)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")  # Add a separator for better readability\n"
      ],
      "metadata": {
        "id": "_-OhGJyD0OiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"Who managed the Destiny's Child group?\"\n",
        "\n",
        "# Iterate through SQuAD v2 examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "sOxElA2asPyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\Graduation project\\\\BERT\\\\BERT fine tuning Q&A SQUAD\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"What would happen without a proper five-year plan?\"\n",
        "\n",
        "# Iterate through SQuAD v2 examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "84Zz8lkrulEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"/content/distilbert-base-uncased-finetuned-squad/checkpoint-2767\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Example question\n",
        "question_example = \"Who's concept of duration was left  behind for a for more concrete frame's of references? \"\n",
        "\n",
        "# Iterate through SQuAD v2 examples to find the context for the example question\n",
        "for example in squad_dataset[\"train\"]:\n",
        "    if question_example.lower() in example[\"question\"].lower():\n",
        "        context = example[\"context\"]\n",
        "        inputs = tokenizer(question_example, context, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "        start_idx = start_logits.argmax(-1).item()\n",
        "        end_idx = end_logits.argmax(-1).item() + 1\n",
        "        answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Question: {question_example}\")\n",
        "        print(f\"Context: {context}\")\n",
        "        print(f\"Answer: {answer}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "33vK2WfaodjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **EVALUATION **\n",
        "\n"
      ],
      "metadata": {
        "id": "Et0T63GKbOvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers scikit-learn\n"
      ],
      "metadata": {
        "id": "NcSvTCDT7uBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from itertools import islice\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"/content/distilbert-base-uncased-finetuned-squad/checkpoint-2767\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Initialize SentenceTransformer for Semantic Answer Similarity (SAS)\n",
        "embedder = SentenceTransformer(\"paraphrase-distilroberta-base-v1\")\n",
        "\n",
        "# List to store SAS scores\n",
        "sas_scores_list = []\n",
        "\n",
        "# Iterate through the first 100 examples in the dataset and compare the generated answer with the ground truth answer\n",
        "for example in islice(squad_dataset[\"train\"], 100):\n",
        "    # Use the model to generate an answer\n",
        "    inputs = tokenizer(example[\"question\"], example[\"context\"], return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    generated_answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "    # Retrieve the ground truth answer from the dataset\n",
        "    ground_truth_answer = example[\"answers\"][\"text\"]\n",
        "\n",
        "    # Calculate Semantic Answer Similarity (SAS) score\n",
        "    try:\n",
        "        texts = [generated_answer, ground_truth_answer]\n",
        "        sas_score = cosine_similarity(embedder.encode(texts))[0][0]\n",
        "        sas_scores_list.append(sas_score)\n",
        "    except IndexError:\n",
        "        print(f\"Error calculating SAS score for texts: {texts}\")\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Question: {example['question']}\")\n",
        "    print(f\"Context: {example['context']}\")\n",
        "    print(f\"Generated Answer: {generated_answer}\")\n",
        "    print(f\"Ground Truth Answer: {ground_truth_answer}\")\n",
        "    print(f\"SAS Score: {sas_score}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# After the loop, you can analyze sas_scores_list as needed\n",
        "average_sas_score = sum(sas_scores_list) / len(sas_scores_list)\n",
        "print(f\"Average SAS Score: {average_sas_score}\")\n"
      ],
      "metadata": {
        "id": "Bm2LmMSA7znk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score\n"
      ],
      "metadata": {
        "id": "KothWwTbFB2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "0bON8yc6LdKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "o8ArDggfLeu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bert_score import score\n",
        "from itertools import islice\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the model checkpoint and load the tokenizer and model\n",
        "model_checkpoint = \"/content/distilbert-base-uncased-finetuned-squad/checkpoint-2767\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load SQuAD v2 dataset\n",
        "squad_dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "# Initialize SentenceTransformer for Semantic Answer Similarity (SAS)\n",
        "embedder = SentenceTransformer(\"paraphrase-distilroberta-base-v1\")\n",
        "\n",
        "# List to store SAS scores\n",
        "sas_scores_list = []\n",
        "em_scores_list = []\n",
        "f1_scores_list = []\n",
        "bert_scores_list = []\n",
        "\n",
        "# Variables for precision and recall\n",
        "total_true_positives = 0\n",
        "total_false_positives = 0\n",
        "total_false_negatives = 0\n",
        "\n",
        "# Function to compute EM (Exact Match) and F1 scores\n",
        "def compute_scores(prediction, reference):\n",
        "    if isinstance(prediction, list):\n",
        "        prediction = ' '.join(prediction)\n",
        "    if isinstance(reference, list):\n",
        "        reference = ' '.join(reference)\n",
        "\n",
        "    prediction_tokens = word_tokenize(prediction.lower())\n",
        "    reference_tokens = word_tokenize(reference.lower())\n",
        "\n",
        "    common = Counter(prediction_tokens) & Counter(reference_tokens)\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    precision = 0.0\n",
        "    recall = 0.0\n",
        "\n",
        "    if num_same > 0:\n",
        "        precision = 1.0 * num_same / len(prediction_tokens)\n",
        "        recall = 1.0 * num_same / len(reference_tokens)\n",
        "\n",
        "    em = int(prediction_tokens == reference_tokens)\n",
        "\n",
        "    return em, precision, recall\n",
        "\n",
        "# Iterate through the first 100 examples in the dataset and compare the generated answer with the ground truth answer\n",
        "for example in islice(squad_dataset[\"train\"], 100):\n",
        "    # Use the model to generate an answer\n",
        "    inputs = tokenizer(example[\"question\"], example[\"context\"], return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    start_idx = start_logits.argmax(-1).item()\n",
        "    end_idx = end_logits.argmax(-1).item() + 1\n",
        "    generated_answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
        "\n",
        "    # Retrieve the ground truth answer from the dataset\n",
        "    if isinstance(example[\"answers\"], dict) and \"answer_start\" in example[\"answers\"]:\n",
        "        ground_truth_answer = example[\"answers\"][\"text\"]\n",
        "    elif example[\"answers\"]:\n",
        "        ground_truth_answer = example[\"answers\"][0][\"text\"]\n",
        "    else:\n",
        "        continue  # Skip examples with no answers\n",
        "\n",
        "    # Calculate Semantic Answer Similarity (SAS) score\n",
        "    try:\n",
        "        texts = [generated_answer, ground_truth_answer]\n",
        "        sas_score = cosine_similarity(embedder.encode(texts))[0][0]\n",
        "        sas_scores_list.append(sas_score)\n",
        "    except IndexError:\n",
        "        print(f\"Error calculating SAS score for texts: {texts}\")\n",
        "\n",
        "    # Compute EM, precision, and recall\n",
        "    em_score, precision, recall = compute_scores(generated_answer, ground_truth_answer)\n",
        "    em_scores_list.append(em_score)\n",
        "\n",
        "    # Update true positives, false positives, and false negatives\n",
        "    total_true_positives += em_score\n",
        "    total_false_positives += int(em_score == 0)\n",
        "    total_false_negatives += int(em_score == 0)\n",
        "\n",
        "    f1_scores_list.append(2 * precision * recall / (precision + recall))\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    bert_scores = score([generated_answer], [ground_truth_answer], lang=\"en\")\n",
        "    bert_score = bert_scores[2].mean().item()\n",
        "    bert_scores_list.append(bert_score)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Question: {example['question']}\")\n",
        "    print(f\"Context: {example['context']}\")\n",
        "    print(f\"Generated Answer: {generated_answer}\")\n",
        "    print(f\"Ground Truth Answer: {ground_truth_answer}\")\n",
        "    print(f\"SAS Score: {sas_score}\")\n",
        "    print(f\"EM Score: {em_score}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {2 * precision * recall / (precision + recall)}\")\n",
        "    print(f\"BERTScore: {bert_score}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# After the loop, you can analyze sas_scores_list, em_scores_list, f1_scores_list, and bert_scores_list as needed\n",
        "average_sas_score = sum(sas_scores_list) / len(sas_scores_list)\n",
        "average_em_score = sum(em_scores_list) / len(em_scores_list)\n",
        "average_f1_score = sum(f1_scores_list) / len(f1_scores_list)\n",
        "average_bert_score = sum(bert_scores_list) / len(bert_scores_list)\n",
        "\n",
        "# Calculate overall precision, recall, and F1\n",
        "overall_precision = total_true_positives / (total_true_positives + total_false_positives)\n",
        "overall_recall = total_true_positives / (total_true_positives + total_false_negatives)\n",
        "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
        "\n",
        "print(f\"Average SAS Score: {average_sas_score}\")\n",
        "print(f\"Average EM Score: {average_em_score}\")\n",
        "print(f\"Average F1 Score: {average_f1_score}\")\n",
        "print(f\"Average BERTScore: {average_bert_score}\")\n",
        "print(f\"Overall Precision: {overall_precision}\")\n",
        "print(f\"Overall Recall: {overall_recall}\")\n",
        "print(f\"Overall F1 Score: {overall_f1}\")\n"
      ],
      "metadata": {
        "id": "oc2GVAdVDTiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Plotting results\n",
        "overall_precision = total_true_positives / (total_true_positives + total_false_positives)\n",
        "overall_recall = total_true_positives / (total_true_positives + total_false_negatives)\n",
        "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
        "\n",
        "# Update metrics and average_scores\n",
        "metrics = ['SAS', 'EM', 'F1', 'BERTScore', 'Precision', 'Recall']\n",
        "average_scores = [average_sas_score, average_em_score, average_f1_score, average_bert_score, overall_precision, overall_recall]\n",
        "\n",
        "# Plotting results\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=metrics, y=average_scores)\n",
        "plt.title('Average Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dwZKt4YWTjXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_scores(scores, metric_name, average_score):\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Scatter plot\n",
        "    plt.scatter(range(1, len(scores) + 1), scores, label=metric_name, marker='o', color='blue', alpha=0.7)\n",
        "\n",
        "    # Highlight exceptional cases (e.g., low scores)\n",
        "    exceptional_indices = [i for i, score in enumerate(scores) if score < 0.5]  # Customize the threshold as needed\n",
        "    plt.scatter(exceptional_indices, [scores[i] for i in exceptional_indices], color='red', label='Exceptional', marker='x')\n",
        "\n",
        "    # Average line\n",
        "    plt.axhline(y=average_score, color='green', linestyle='--', label=f'Average {metric_name}')\n",
        "\n",
        "    plt.title(f'{metric_name} Scores for Each Example')\n",
        "    plt.xlabel('Example Index')\n",
        "    plt.ylabel(f'{metric_name} score')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting results for each metric\n",
        "average_sas_score = sum(sas_scores_list) / len(sas_scores_list)\n",
        "average_em_score = sum(em_scores_list) / len(em_scores_list)\n",
        "average_f1_score = sum(f1_scores_list) / len(f1_scores_list)\n",
        "average_bert_score = sum(bert_scores_list) / len(bert_scores_list)\n",
        "\n",
        "plot_scores(sas_scores_list, 'SAS', average_sas_score)\n",
        "plot_scores(em_scores_list, 'EM', average_em_score)\n",
        "plot_scores(f1_scores_list, 'F1', average_f1_score)\n",
        "plot_scores(bert_scores_list, 'BERTScore', average_bert_score)"
      ],
      "metadata": {
        "id": "myquGglcFd2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You can now upload the result of the training to the Hub, just execute this instruction:**"
      ],
      "metadata": {
        "id": "rci2EnBLLpRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Check if the model exists\n",
        "model_name = \"distilbert-base-uncased-finetuned-squad\"\n",
        "try:\n",
        "    pipeline(model=model_name, tokenizer=model_name)\n",
        "    print(f\"The model {model_name} exists and is accessible.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "4LKgtmZLv-7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-squad\"\n",
        "\n",
        "try:\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    print(\"Config file (config.json) exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"Config file (config.json) is missing. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "4TqiVM9qx4jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "config_content = {\n",
        "    \"architectures\": [\"DistilBertForQuestionAnswering\"],\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"num_labels\": 2,\n",
        "    \"id2label\": {\"0\": \"LABEL_0\", \"1\": \"LABEL_1\"},\n",
        "    \"label2id\": {\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
        "}\n",
        "\n",
        "# Use the current working directory as the output directory\n",
        "output_directory = os.getcwd()\n",
        "config_path = os.path.join(output_directory, \"config.json\")\n",
        "\n",
        "# Write the content to the config.json file\n",
        "with open(config_path, \"w\") as config_file:\n",
        "    json.dump(config_content, config_file, indent=4)\n",
        "\n",
        "print(f\"Config file (config.json) has been created at: {config_path}\")\n"
      ],
      "metadata": {
        "id": "SATSBUe41Ojf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNGA3y8K9LjN"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87WK7UY99LjN"
      },
      "source": [
        "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"sgugger/my-awesome-model\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9whIBKlr9LjO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "rEJBSTyZIrIb",
        "whPRbBNbIrIl",
        "n9qywopnIrJH",
        "545PP3o8IrJV",
        "XN3XpeWJjeZH",
        "tKzIg5qejlJj",
        "_QqGzIb89LjI",
        "F2qXoTRVYNuT",
        "Et0T63GKbOvH",
        "rci2EnBLLpRb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bd00e1b2f51422fa5f78a35eef57398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b2c077d2bd94dfbbb4145883c1ed910",
              "IPY_MODEL_adafffc9691449199c756c4d05d3d39a",
              "IPY_MODEL_f5cd929b520d4836a2ced29e2b7848a8",
              "IPY_MODEL_a55d8b7e8e054239a02994bc376baa01",
              "IPY_MODEL_3334b4f86aec4a9cb0563e1351ba21f7"
            ],
            "layout": "IPY_MODEL_295ec0ddab7c4da3a9a1a3386240073f"
          }
        },
        "8b2c077d2bd94dfbbb4145883c1ed910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a7d73179da94df2a3213c6dba390c5b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5bc72a6d8908461ea33150e6a66ebb85",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "adafffc9691449199c756c4d05d3d39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_bd5fc16ccfd3428daf3961a0a787fa04",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_65b14a6fef2b438790f0290e6d35896c",
            "value": ""
          }
        },
        "f5cd929b520d4836a2ced29e2b7848a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_46352aced18f44d2832a2435ca576c92",
            "style": "IPY_MODEL_89570a66c82544be86906fd3981babb1",
            "value": true
          }
        },
        "a55d8b7e8e054239a02994bc376baa01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2cb2b417cb844ff88d02fd0a9190d155",
            "style": "IPY_MODEL_21b820301a0b43d18cdbea0fe75ccf6d",
            "tooltip": ""
          }
        },
        "3334b4f86aec4a9cb0563e1351ba21f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe7326f283cc4eed869da533f5d523a4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f8bd4645964b41e9872610dbe10440c3",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "295ec0ddab7c4da3a9a1a3386240073f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "5a7d73179da94df2a3213c6dba390c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bc72a6d8908461ea33150e6a66ebb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd5fc16ccfd3428daf3961a0a787fa04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b14a6fef2b438790f0290e6d35896c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46352aced18f44d2832a2435ca576c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89570a66c82544be86906fd3981babb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cb2b417cb844ff88d02fd0a9190d155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b820301a0b43d18cdbea0fe75ccf6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "fe7326f283cc4eed869da533f5d523a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8bd4645964b41e9872610dbe10440c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d233451031f47568977398a28084f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c2868c3680146218c02d53cf86c1ea7",
              "IPY_MODEL_e84df80b8c7d4b32a3aae8171b2f395c",
              "IPY_MODEL_8461d79141624c848273ac5b4274ee12"
            ],
            "layout": "IPY_MODEL_26b6a33026d741c1a5cb039e619ddc2a"
          }
        },
        "4c2868c3680146218c02d53cf86c1ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a94874796048418100d4bafdee93e2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3893f773067246019513c4a2fbd22647",
            "value": "Downloading readme: 100%"
          }
        },
        "e84df80b8c7d4b32a3aae8171b2f395c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c465156f43543438a5e3c30ae66ba49",
            "max": 7832,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8d51741c6ff48d6b275b258b22873df",
            "value": 7832
          }
        },
        "8461d79141624c848273ac5b4274ee12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e498e9d6f946e1abf85f1f53e4592b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_33b69afd955a4c098e91b07d03935987",
            "value": " 7.83k/7.83k [00:00&lt;00:00, 368kB/s]"
          }
        },
        "26b6a33026d741c1a5cb039e619ddc2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a94874796048418100d4bafdee93e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3893f773067246019513c4a2fbd22647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c465156f43543438a5e3c30ae66ba49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d51741c6ff48d6b275b258b22873df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87e498e9d6f946e1abf85f1f53e4592b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33b69afd955a4c098e91b07d03935987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76eeb23a15784f148fbf2386e9bda6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bbfd8c093a14a1f85af59bf59d3098c",
              "IPY_MODEL_49f44fcdfc6b4e1b834f255ff2aa0c72",
              "IPY_MODEL_3922c3ee6fe244a0abf6d97523fd472b"
            ],
            "layout": "IPY_MODEL_7a7198148d7d41279c7b8110ad3d3f08"
          }
        },
        "6bbfd8c093a14a1f85af59bf59d3098c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c09f24d4d9194569a014ebbee203da9f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1ff2a96807a045448ff16640f5abdb8e",
            "value": "Downloading data: 100%"
          }
        },
        "49f44fcdfc6b4e1b834f255ff2aa0c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff1a188d072841b88756d7e4b6da1267",
            "max": 14458314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ce772b4096c49978dac88bf314e43fb",
            "value": 14458314
          }
        },
        "3922c3ee6fe244a0abf6d97523fd472b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81868ae800154ce082e0e044148bb810",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c5261fb25c7c4b349d67572ec808ba5d",
            "value": " 14.5M/14.5M [00:00&lt;00:00, 19.2MB/s]"
          }
        },
        "7a7198148d7d41279c7b8110ad3d3f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c09f24d4d9194569a014ebbee203da9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff2a96807a045448ff16640f5abdb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff1a188d072841b88756d7e4b6da1267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce772b4096c49978dac88bf314e43fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81868ae800154ce082e0e044148bb810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5261fb25c7c4b349d67572ec808ba5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8683a37b81af47e9af1bcf90b5fca885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c57e8a119de4343af0b97e7c1311718",
              "IPY_MODEL_42d7989a4d974413a1a22155376ca1bc",
              "IPY_MODEL_835869141a9d4c8d9506d0234ab21fca"
            ],
            "layout": "IPY_MODEL_5b2918bb8d404a65967484a9db1359d9"
          }
        },
        "5c57e8a119de4343af0b97e7c1311718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7584bab1611e4cc0a1a45aa4660ad617",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_29fb41a96fbe410f9630a46fda015419",
            "value": "Downloading data: 100%"
          }
        },
        "42d7989a4d974413a1a22155376ca1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6684f59fa31451b89a2ef4da4a1b503",
            "max": 1819889,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6afb9247d924be09ebfefadab0de3cf",
            "value": 1819889
          }
        },
        "835869141a9d4c8d9506d0234ab21fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5756765f3b94af0b7cae88645722cae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ce784daeb0ba454c943a6213b5ecadc4",
            "value": " 1.82M/1.82M [00:00&lt;00:00, 5.90MB/s]"
          }
        },
        "5b2918bb8d404a65967484a9db1359d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7584bab1611e4cc0a1a45aa4660ad617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29fb41a96fbe410f9630a46fda015419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6684f59fa31451b89a2ef4da4a1b503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6afb9247d924be09ebfefadab0de3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5756765f3b94af0b7cae88645722cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce784daeb0ba454c943a6213b5ecadc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ba81c93e9f24168aff805d25e3777cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_698f7b9e320146229e4582d374d1dbc0",
              "IPY_MODEL_44e58997083348a5845f6907c1285d8f",
              "IPY_MODEL_6199c87fd1684b70b75635e989509f3b"
            ],
            "layout": "IPY_MODEL_a2c2ec61c6ca4149be4b3bd1aae1f4ba"
          }
        },
        "698f7b9e320146229e4582d374d1dbc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca34dec26eaf492f93fe8d4da68691c9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0ba1b25268e24aab8d372f8f1d8e44c0",
            "value": "Generating train split: 100%"
          }
        },
        "44e58997083348a5845f6907c1285d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_688375edee6d4b2cac827834efa316a7",
            "max": 87599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2c4d3a763584e8d99dbde67072f4c5b",
            "value": 87599
          }
        },
        "6199c87fd1684b70b75635e989509f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0f0c560e1034803b7aa33fb341e53bb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_854715dfeda945f7bb33cf5110d939cb",
            "value": " 87599/87599 [00:00&lt;00:00, 279087.38 examples/s]"
          }
        },
        "a2c2ec61c6ca4149be4b3bd1aae1f4ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca34dec26eaf492f93fe8d4da68691c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba1b25268e24aab8d372f8f1d8e44c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688375edee6d4b2cac827834efa316a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c4d3a763584e8d99dbde67072f4c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0f0c560e1034803b7aa33fb341e53bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "854715dfeda945f7bb33cf5110d939cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "915c9a826c2742f8939b0b522331f55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_092304d802ac4b9aba4dd6dfcbaee109",
              "IPY_MODEL_a96680260d3f42db9bcb58958d808c27",
              "IPY_MODEL_8ade84e485e64c33a110b3fa85651da8"
            ],
            "layout": "IPY_MODEL_a7249be0aba94974b1016af4cc9298ef"
          }
        },
        "092304d802ac4b9aba4dd6dfcbaee109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6013fbfd434ee690362d58b231223c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f81a5f6b500c41c38f84ae5ee71aa5b5",
            "value": "Generating validation split: 100%"
          }
        },
        "a96680260d3f42db9bcb58958d808c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c9f461ca1814681bafe115111d0e98b",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8c5cfcc725e45ee888b20a615b038ad",
            "value": 10570
          }
        },
        "8ade84e485e64c33a110b3fa85651da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e21e5b472c4d406f832e0916d8caf287",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3548db6299014ed49e074f6621dfa74e",
            "value": " 10570/10570 [00:00&lt;00:00, 76325.16 examples/s]"
          }
        },
        "a7249be0aba94974b1016af4cc9298ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a6013fbfd434ee690362d58b231223c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81a5f6b500c41c38f84ae5ee71aa5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c9f461ca1814681bafe115111d0e98b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c5cfcc725e45ee888b20a615b038ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e21e5b472c4d406f832e0916d8caf287": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3548db6299014ed49e074f6621dfa74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60b026e48c7c4d50982b0e8b05561511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c6da572ef104a2db7720ee78e2cf1f0",
              "IPY_MODEL_475c460e52e44aad9dc30e65c3012bfc",
              "IPY_MODEL_ecf72ba15acc4cbe9a777f5d21044598"
            ],
            "layout": "IPY_MODEL_1ca6e16388c340f9b31c4c3c3a257b0d"
          }
        },
        "3c6da572ef104a2db7720ee78e2cf1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05f16b669b1447c7a88c4e2b6f713fd4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_09987b8065f043388f9704cbdced823a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "475c460e52e44aad9dc30e65c3012bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ea716730fc0433294892d7e18bf6b3e",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6ebbdfbe91e440487b269264038f528",
            "value": 28
          }
        },
        "ecf72ba15acc4cbe9a777f5d21044598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aca8d6b845ac4c0f8956dde53dcb06aa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8713156648574a6e99cc99479a950320",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.06kB/s]"
          }
        },
        "1ca6e16388c340f9b31c4c3c3a257b0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f16b669b1447c7a88c4e2b6f713fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09987b8065f043388f9704cbdced823a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ea716730fc0433294892d7e18bf6b3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ebbdfbe91e440487b269264038f528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aca8d6b845ac4c0f8956dde53dcb06aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8713156648574a6e99cc99479a950320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "683f7b2f3bba49128ee676fb86e0b874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64f64ed846ea47aeaa8d856a03eddfc4",
              "IPY_MODEL_b30c9b88c1884661b7b8d8471efee5ed",
              "IPY_MODEL_e61c11ab9dff421ba36169faad5f14b4"
            ],
            "layout": "IPY_MODEL_acc509904cbf4e098d5db5256d0ffc6d"
          }
        },
        "64f64ed846ea47aeaa8d856a03eddfc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b68285265bf4ffea3e257ed59ffeb3f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_29ea44a691e845b78e40d0c53fb42d1f",
            "value": "config.json: 100%"
          }
        },
        "b30c9b88c1884661b7b8d8471efee5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a485ca524ad74ad294f3ca2a7deca63c",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_590abea13c484a65a68331fde8ef7a83",
            "value": 483
          }
        },
        "e61c11ab9dff421ba36169faad5f14b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f55922252ea475b80c40c14319c86b5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a819e10fe5004940aadf364b9e8aced4",
            "value": " 483/483 [00:00&lt;00:00, 7.45kB/s]"
          }
        },
        "acc509904cbf4e098d5db5256d0ffc6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b68285265bf4ffea3e257ed59ffeb3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ea44a691e845b78e40d0c53fb42d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a485ca524ad74ad294f3ca2a7deca63c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "590abea13c484a65a68331fde8ef7a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f55922252ea475b80c40c14319c86b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a819e10fe5004940aadf364b9e8aced4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7cab69bdb5d4590abda56cab691c0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3765a0df35144b74aad4c2d0ec9fd7a2",
              "IPY_MODEL_98eea8b2bb614e98898170cbbe9ebed9",
              "IPY_MODEL_53e685182d464b9b9c4eed74d537ddd0"
            ],
            "layout": "IPY_MODEL_65ec33da2b5f4952895ac4c6cce06c2d"
          }
        },
        "3765a0df35144b74aad4c2d0ec9fd7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61ef2a982e0c4a6e859eb40195461653",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0e1d21da0fcb4619a7201f79054e9ef2",
            "value": "vocab.txt: 100%"
          }
        },
        "98eea8b2bb614e98898170cbbe9ebed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6f2024cae404f4d8938f2fce1aba945",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ece9e92a15549c0bc2a92a17d8238d2",
            "value": 231508
          }
        },
        "53e685182d464b9b9c4eed74d537ddd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a306353b23bf49958b62ae68d137075e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e77f5a2641da4edd9f56e78a99763d48",
            "value": " 232k/232k [00:00&lt;00:00, 2.52MB/s]"
          }
        },
        "65ec33da2b5f4952895ac4c6cce06c2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61ef2a982e0c4a6e859eb40195461653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e1d21da0fcb4619a7201f79054e9ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6f2024cae404f4d8938f2fce1aba945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ece9e92a15549c0bc2a92a17d8238d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a306353b23bf49958b62ae68d137075e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e77f5a2641da4edd9f56e78a99763d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "969076ead6c04bdc910e17c632d36000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4a3471e733b4099ad64be27b4690850",
              "IPY_MODEL_64d1d051a42742818530aaa329a1b57c",
              "IPY_MODEL_7ce573546edc4953b1f958efb4907068"
            ],
            "layout": "IPY_MODEL_958f45d2fab74b9b9343ee2c995c91ca"
          }
        },
        "d4a3471e733b4099ad64be27b4690850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_433f2a42134f43668313a94344bfc3ae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36201a96150e46ee8758d161ec79a359",
            "value": "tokenizer.json: 100%"
          }
        },
        "64d1d051a42742818530aaa329a1b57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cca8b5990a7944d9b451b09bd9cb37d7",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a71c9cd217cf4795b8377463548721fe",
            "value": 466062
          }
        },
        "7ce573546edc4953b1f958efb4907068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86137c75a8484e0f9e33e39efdd40ba1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_20987be84227409b890548d4ea15441b",
            "value": " 466k/466k [00:00&lt;00:00, 9.93MB/s]"
          }
        },
        "958f45d2fab74b9b9343ee2c995c91ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "433f2a42134f43668313a94344bfc3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36201a96150e46ee8758d161ec79a359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cca8b5990a7944d9b451b09bd9cb37d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71c9cd217cf4795b8377463548721fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86137c75a8484e0f9e33e39efdd40ba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20987be84227409b890548d4ea15441b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4bd895d71744bbc82b40803554269bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d01f5e67e9334334be0ff5f188829c2e",
              "IPY_MODEL_4aac69f8d94146f994286bca547d788f",
              "IPY_MODEL_87d5095234304e0ea4f3f99a310bcd55"
            ],
            "layout": "IPY_MODEL_8851916313f4475e94b972fe096c800e"
          }
        },
        "d01f5e67e9334334be0ff5f188829c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb50d0f182fc48f28f8211677715eed2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f22995e46aba47b1816dc98d6a932eb0",
            "value": "Map:  58%"
          }
        },
        "4aac69f8d94146f994286bca547d788f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761f312ad1784e58b0ee2dfd0bc47f7b",
            "max": 87599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f26eca8df825485b94c40cbebdc43da9",
            "value": 51000
          }
        },
        "87d5095234304e0ea4f3f99a310bcd55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0913bb55fe364b9b958727f98d41ac4e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6e6fc322874d444b83803b0f1861f1f3",
            "value": " 51000/87599 [01:16&lt;00:31, 1166.03 examples/s]"
          }
        },
        "8851916313f4475e94b972fe096c800e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb50d0f182fc48f28f8211677715eed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22995e46aba47b1816dc98d6a932eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "761f312ad1784e58b0ee2dfd0bc47f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26eca8df825485b94c40cbebdc43da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0913bb55fe364b9b958727f98d41ac4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e6fc322874d444b83803b0f1861f1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}